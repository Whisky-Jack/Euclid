{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from itertools import count\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "\"\"\"\n",
    "Actions:\n",
    "    -Draw vertex\n",
    "    -Measure distance\n",
    "    -Measure angle\n",
    "\"\"\"\n",
    "# DEFINE ENVIRONMENT\n",
    "class GeometryEnvironment():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def place_vertex(self, bead_index, vertex_index):\n",
    "        self.bead_set[bead_index] = vertex_index\n",
    "\n",
    "    def measure_distance(self, u, v):\n",
    "        return euclidean(u, v)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.bead_set = np.array([(randint(low=1, high=100), randint(low=1, high=100)) for i in range(2)])\n",
    "        self.defining_state = (randint(low=1, high=100), randint(low=1, high=100))\n",
    "        self.answer = 0\n",
    "        self.correct_answer = np.sqrt(self.defining_state[0]**2 + self.defining_state[1]**2)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # if distance should be measured\n",
    "        #print(\"beads at start of step\", self.bead_set)\n",
    "        # convert tensor to format which can be manipulated\n",
    "        if (action.requires_grad):\n",
    "            action = action.clone().detach().numpy().T\n",
    "        action_index = action[0]\n",
    "        action_vertex = (action[1], action[2])\n",
    "        if (action_index < -5):\n",
    "            self.answer = self.measure_distance(self.bead_set[0], self.bead_set[1])\n",
    "        else:\n",
    "            action_index = 0 if action_index < 5 else 1\n",
    "            self.place_vertex(action_index, action_vertex)\n",
    "        reward = 1 / (np.abs(self.answer - self.correct_answer)**2)\n",
    "        done = self.answer == self.correct_answer\n",
    "        #print(\"beads at end of step\", self.bead_set)\n",
    "        #print(\"distance: \", self.measure_distance(self.bead_set[0], self.bead_set[1]))\n",
    "        return self.get_state(), reward, done, -1\n",
    "    \n",
    "    def get_state(self):\n",
    "        a = np.array(self.defining_state)\n",
    "        b = np.array(self.bead_set[0])\n",
    "        c = np.array(self.bead_set[1])\n",
    "        nump = np.concatenate((a, b, c))\n",
    "        t_state = torch.tensor(nump, dtype=torch.long)\n",
    "        t_state = t_state.float().flatten()\n",
    "        t_state.requires_grad = True\n",
    "        return t_state\n",
    "    \n",
    "    def render(self):\n",
    "        #print(\"Answer: \", self.answer)\n",
    "        vertices = np.array([self.vertex_set[(self.bead_set[0])], self.vertex_set[(self.bead_set[1])]])\n",
    "        #print(vertices)\n",
    "        x, y = vertices.T\n",
    "        plt.scatter(x, y)\n",
    "        plt.show()\n",
    "    \n",
    "    def sample_action_space(self):\n",
    "        action_type = float(random.randint(0, 2))\n",
    "        return torch.tensor([action_type, float(randint(low=1, high=100)), float(randint(low=1, high=100))], requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLAY MEMORY DEFINITION\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems:\n",
    "\n",
    "-Need to provide float args to vertex position (x and y)\n",
    "\n",
    "-Need to provide index argument to vertex position\n",
    "\n",
    "-Need to provide index arguments to distance measurement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE NETWORK\n",
    "# TODO: replace CNN with linear learner\n",
    "\n",
    "def print_with_extreme_prejudice(num):\n",
    "    print(\"Fuck you you fucking piece of shit\", num)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dimension, output_dimension):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dimension, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.head = nn.Linear(32, output_dimension)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization\n",
    "    # Returns a distribution over the possible actions tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        self.out = self.head(x.view(1, x.size(0)))[0]\n",
    "        return  self.out # self.head(x.view(x.size(0), -1))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dimension, output_dimension):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(input_dimension, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.head = nn.Linear(32, output_dimension)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.state_input = torch.randn((1, 6), requires_grad=True)\n",
    "\n",
    "        self.state1 = nn.Linear(6, 32)  # 6*6 from image dimension\n",
    "        self.state2 = nn.Linear(32, 64)\n",
    "\n",
    "        self.action_input = torch.randn((1, 3), requires_grad=True)\n",
    "        self.action_input.retain_grad()\n",
    "        self.action1 = nn.Linear(3, 32)  # 6*6 from image dimension\n",
    "\n",
    "        self.merged1 = nn.Linear(96, 32)\n",
    "\n",
    "        self.head = nn.Linear(32, output_dimension)\n",
    "        self.out = torch.randn((1), requires_grad=True)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization\n",
    "    # Returns a distribution over the possible actions tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        self.action_input.register_hook(print_with_extreme_prejudice)\n",
    "        x = x.float()\n",
    "        #print(x.shape)\n",
    "        self.state_input = x[0:6].clone()\n",
    "        x_state = F.relu(self.state1(self.state_input))\n",
    "        x_state = F.relu(self.state2(x_state))\n",
    "\n",
    "        self.action_input = x[6:10].clone()\n",
    "        x_action = F.relu(self.action1(self.action_input))\n",
    "\n",
    "        x = torch.cat((x_state, x_action))\n",
    "        x = F.relu(self.merged1(x))\n",
    "        self.out = self.head(x.view(-1, x.size(0))) # self.head(x.view(x.size(0), -1))\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "def clone_tensors(item_list):\n",
    "    new_items = []\n",
    "    for item in item_list:\n",
    "        new_items.append(item.clone() if torch.is_tensor(item) else item)\n",
    "    return new_items\n",
    "\n",
    "class ActorCritic:\n",
    "    def __init__(self, env):\n",
    "        self.env  = env\n",
    "\n",
    "        self.action_dimension = 3\n",
    "        self.environment_dimension = 6     \n",
    "        \n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = .995\n",
    "        self.gamma = .95\n",
    "        self.tau   = .125\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # ===================================================================== #\n",
    "        #                               Actor Model                             #\n",
    "        # Chain rule: find the gradient of changing the actor network params in #\n",
    "        # getting closest to the final value network predictions, i.e. de/dA    #\n",
    "        # Calculate de/dA as = de/dC * dC/dA, where e is error, C critic, A act #\n",
    "        # ===================================================================== #\n",
    "\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        self.actor_model = Actor(self.environment_dimension, self.action_dimension).float()\n",
    "        self.target_actor_model = Actor(self.environment_dimension, self.action_dimension).float()\n",
    "\n",
    "        self.target_actor_model.load_state_dict(self.actor_model.state_dict())\n",
    "        self.target_actor_model.eval()\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor_model.parameters())\n",
    "        \n",
    "        #print(self.actor_model.state_dict())\n",
    "        \"\"\"\n",
    "        self.actor_critic_grad = tf.placeholder(tf.float32, \n",
    "            [None, self.env.action_space.shape[0]]) # where we will feed de/dC (from critic)\n",
    "        \n",
    "        actor_model_weights = self.actor_model.trainable_weights\n",
    "        self.actor_grads = tf.gradients(self.actor_model.output, \n",
    "            actor_model_weights, -self.actor_critic_grad) # dC/dA (from actor)\n",
    "        \n",
    "        grads = zip(self.actor_grads, actor_model_weights)\n",
    "\n",
    "        \n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)\n",
    "         optimizer = optim.Adam(policy_net.parameters())\n",
    "         \"\"\"\n",
    "\n",
    "        # ===================================================================== #\n",
    "        #                              Critic Model                             #\n",
    "        # ===================================================================== #        \n",
    "\n",
    "        self.critic_model = Critic(self.action_dimension + self.environment_dimension, 1).float()\n",
    "        self.target_critic_model = Critic(self.action_dimension + self.environment_dimension, 1).float()\n",
    "\n",
    "        self.target_critic_model.load_state_dict(self.critic_model.state_dict())\n",
    "        self.target_critic_model.eval()\n",
    "\n",
    "        #self.critic_grads =  torch.autograd.grad(self.critic_model.head, self.critic_model.action_input)\n",
    "\n",
    "        self.critic_optimizer = optim.Adam(self.critic_model.parameters())\n",
    "        #self.critic_optimizer = optim.RMSprop(self.actor_model.parameters())\n",
    "\n",
    "        \"\"\"\n",
    "        self.critic_grads = tf.gradients(self.critic_model.output, self.critic_action_input) # where we calcaulte de/        dC for feeding above\n",
    "        optimizer = optim.Adam(policy_net.parameters())\n",
    "        \"\"\"\n",
    "        \n",
    "    # ===================================================================== #\n",
    "    #                              Training                                 #\n",
    "    # ===================================================================== #    \n",
    "\n",
    "    def remember(self, cur_state, action, reward, new_state, done):\n",
    "        self.memory.append([cur_state, action, reward, new_state, done])\n",
    "    \n",
    "    def _train_actor(self, samples):\n",
    "        for real_sample in samples:\n",
    "            sample = clone_tensors(real_sample)\n",
    "            # Want to get critic grad, but with respect to the predicted action as the action input\n",
    "            cur_state, action, reward, new_state, _ = sample\n",
    "\n",
    "            # get action input\n",
    "            #self.actor_optimizer.zero_grad()\n",
    "            predicted_action = self.actor_model(cur_state)\n",
    "            \n",
    "            # get critic predictions\n",
    "            critic_input = torch.cat((cur_state, predicted_action))\n",
    "            expected_rewards = self.critic_model(critic_input)\n",
    "\n",
    "            prediction_critic_grad = torch.autograd.grad(self.critic_model.out, self.critic_model.action_input, grad_outputs=expected_rewards)[0].clone()\n",
    "\n",
    "            actor_model_weights = self.actor_model.parameters()\n",
    "            \n",
    "            # Compute actor gradients\n",
    "\n",
    "            self.actor_grads = torch.autograd.grad(self.actor_model.out, actor_model_weights, grad_outputs=-prediction_critic_grad)\n",
    "\n",
    "            # Set up relation between actor grads and weights\n",
    "            zipperino = zip(list(self.actor_model.parameters()), self.actor_grads)\n",
    "            \n",
    "            for param, gradient in zipperino:\n",
    "                param.grad = gradient.clone()\n",
    "            \n",
    "            # Step optimizer\n",
    "            #self.actor_optimizer.step()\n",
    "            \n",
    "    def _train_critic(self, samples):\n",
    "        for real_sample in samples:\n",
    "            sample = clone_tensors(real_sample)\n",
    "            cur_state, action, reward, new_state, done = sample\n",
    "            if not done:\n",
    "                target_action = self.target_actor_model(new_state)\n",
    "                # include predicted future reward in reward\n",
    "                #print(target_action)\n",
    "                critic_input = torch.cat((new_state.float(), target_action), 0)\n",
    "                future_reward = self.target_critic_model(critic_input)[0]\n",
    "                reward += self.gamma * future_reward\n",
    "            # train model\n",
    "            optim_critic_input = torch.cat((cur_state.float(), action.float()))\n",
    "            self.optimize_critic(optim_critic_input, reward)\n",
    "    \n",
    "    def optimize_critic(self, input_state, reward):\n",
    "        # optimize critic on a batch\n",
    "\n",
    "        # get critic predictions\n",
    "        expected_rewards = self.critic_model(input_state)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.smooth_l1_loss(expected_rewards, reward.unsqueeze(1))\n",
    "        print(loss)\n",
    "\n",
    "\n",
    "        #self.critic_model.action_input.retain_grad()\n",
    "        #loss = loss.double()\n",
    "        loss.backward(retain_graph=True)\n",
    "        #print(\"Fuck: \", self.critic_model.action_input.grad)\n",
    "        self.critic_grad = torch.autograd.grad(self.critic_model.out, self.critic_model.action_input)[0]\n",
    "\n",
    "        for param in self.critic_model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        rewards = []\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        self._train_critic(samples)\n",
    "        self._train_actor(samples)\n",
    "\n",
    "    # ========================================================================= #\n",
    "    #                         Target Model Updating                             #\n",
    "    # ========================================================================= #\n",
    "\n",
    "    def _update_actor_target(self):\n",
    "        self.target_actor_model.load_state_dict(self.actor_model.state_dict())\n",
    "\n",
    "    def _update_critic_target(self):\n",
    "        self.target_critic_model.load_state_dict(self.critic_model.state_dict())\n",
    "\n",
    "    def update_target(self):\n",
    "        self._update_actor_target()\n",
    "        self._update_critic_target()\n",
    "\n",
    "    # ========================================================================= #\n",
    "    #                              Model Predictions                            #\n",
    "    # ========================================================================= #\n",
    "\n",
    "    def act(self, cur_state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.sample_action_space()\n",
    "        \n",
    "        return self.actor_model(cur_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(False)\n",
    "def main():\n",
    "    env = GeometryEnvironment()\n",
    "    actor_critic = ActorCritic(env)\n",
    "\n",
    "    num_trials = 10000\n",
    "    trial_len  = 500\n",
    "\n",
    "    cur_state = env.reset()\n",
    "    action = env.sample_action_space()\n",
    "    count = 0\n",
    "    while True:\n",
    "        #env.render()\n",
    "        print(count)\n",
    "        count += 1\n",
    "        cur_state = env.get_state()\n",
    "\n",
    "        action = actor_critic.act(cur_state)\n",
    "        #action = action[0] #action.reshape((1, env.action_space.shape[0]))\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        #new_state = new_state.reshape((1, env.observation_space.shape[0]))\n",
    "\n",
    "        actor_critic.remember(cur_state, action, reward, new_state, done)\n",
    "        actor_critic.train()\n",
    "\n",
    "        cur_state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\ntensor(3.7039, grad_fn=<MeanBackward0>)\ntensor(1.0810, grad_fn=<MeanBackward0>)\ntensor(0.0678, grad_fn=<MeanBackward0>)\ntensor(0.8101, grad_fn=<MeanBackward0>)\ntensor(1.6264, grad_fn=<MeanBackward0>)\ntensor(0.1501, grad_fn=<MeanBackward0>)\ntensor(0.2737, grad_fn=<MeanBackward0>)\ntensor(0.0001, grad_fn=<MeanBackward0>)\ntensor(1.8348, grad_fn=<MeanBackward0>)\ntensor(0.1478, grad_fn=<MeanBackward0>)\ntensor(0.0896, grad_fn=<MeanBackward0>)\ntensor(0.0252, grad_fn=<MeanBackward0>)\ntensor(0.6493, grad_fn=<MeanBackward0>)\ntensor(2.0303, grad_fn=<MeanBackward0>)\ntensor(0.2711, grad_fn=<MeanBackward0>)\ntensor(0.2241, grad_fn=<MeanBackward0>)\ntensor(0.6136, grad_fn=<MeanBackward0>)\ntensor(0.1189, grad_fn=<MeanBackward0>)\ntensor(0.5221, grad_fn=<MeanBackward0>)\ntensor(0.1343, grad_fn=<MeanBackward0>)\ntensor(0.0248, grad_fn=<MeanBackward0>)\ntensor(0.2510, grad_fn=<MeanBackward0>)\ntensor(0.6752, grad_fn=<MeanBackward0>)\ntensor(0.4778, grad_fn=<MeanBackward0>)\ntensor(0.0401, grad_fn=<MeanBackward0>)\ntensor(0.1599, grad_fn=<MeanBackward0>)\ntensor(0.0150, grad_fn=<MeanBackward0>)\ntensor(0.2457, grad_fn=<MeanBackward0>)\ntensor(0.1252, grad_fn=<MeanBackward0>)\ntensor(0.2791, grad_fn=<MeanBackward0>)\ntensor(0.0395, grad_fn=<MeanBackward0>)\ntensor(0.4927, grad_fn=<MeanBackward0>)\n32\ntensor(0.9681, grad_fn=<MeanBackward0>)\ntensor(0.0165, grad_fn=<MeanBackward0>)\ntensor(0.0031, grad_fn=<MeanBackward0>)\ntensor(0.0350, grad_fn=<MeanBackward0>)\ntensor(0.0286, grad_fn=<MeanBackward0>)\ntensor(0.0906, grad_fn=<MeanBackward0>)\ntensor(0.0255, grad_fn=<MeanBackward0>)\ntensor(0.0460, grad_fn=<MeanBackward0>)\ntensor(0.0465, grad_fn=<MeanBackward0>)\ntensor(0.0878, grad_fn=<MeanBackward0>)\ntensor(0.0248, grad_fn=<MeanBackward0>)\ntensor(0.0686, grad_fn=<MeanBackward0>)\ntensor(0.8296, grad_fn=<MeanBackward0>)\ntensor(0.1079, grad_fn=<MeanBackward0>)\ntensor(0.1952, grad_fn=<MeanBackward0>)\ntensor(0.0242, grad_fn=<MeanBackward0>)\ntensor(0.0018, grad_fn=<MeanBackward0>)\ntensor(0.0216, grad_fn=<MeanBackward0>)\ntensor(0.3399, grad_fn=<MeanBackward0>)\ntensor(0.0334, grad_fn=<MeanBackward0>)\ntensor(0.0285, grad_fn=<MeanBackward0>)\ntensor(0.0306, grad_fn=<MeanBackward0>)\ntensor(0.0138, grad_fn=<MeanBackward0>)\ntensor(0.0216, grad_fn=<MeanBackward0>)\ntensor(0.0042, grad_fn=<MeanBackward0>)\ntensor(0.0236, grad_fn=<MeanBackward0>)\ntensor(0.1061, grad_fn=<MeanBackward0>)\ntensor(0.0510, grad_fn=<MeanBackward0>)\ntensor(0.0160, grad_fn=<MeanBackward0>)\ntensor(0.0079, grad_fn=<MeanBackward0>)\ntensor(0.0397, grad_fn=<MeanBackward0>)\ntensor(0.0051, grad_fn=<MeanBackward0>)\n33\ntensor(0.0082, grad_fn=<MeanBackward0>)\ntensor(0.0161, grad_fn=<MeanBackward0>)\ntensor(0.0075, grad_fn=<MeanBackward0>)\ntensor(0.0343, grad_fn=<MeanBackward0>)\ntensor(0.0445, grad_fn=<MeanBackward0>)\ntensor(0.0265, grad_fn=<MeanBackward0>)\ntensor(0.0252, grad_fn=<MeanBackward0>)\ntensor(0.0066, grad_fn=<MeanBackward0>)\ntensor(0.0610, grad_fn=<MeanBackward0>)\ntensor(0.1259, grad_fn=<MeanBackward0>)\ntensor(0.0112, grad_fn=<MeanBackward0>)\ntensor(0.1696, grad_fn=<MeanBackward0>)\ntensor(0.0227, grad_fn=<MeanBackward0>)\ntensor(0.0501, grad_fn=<MeanBackward0>)\ntensor(0.0639, grad_fn=<MeanBackward0>)\ntensor(0.1006, grad_fn=<MeanBackward0>)\ntensor(0.0364, grad_fn=<MeanBackward0>)\ntensor(0.0984, grad_fn=<MeanBackward0>)\ntensor(0.3002, grad_fn=<MeanBackward0>)\ntensor(0.0793, grad_fn=<MeanBackward0>)\ntensor(0.0546, grad_fn=<MeanBackward0>)\ntensor(0.0315, grad_fn=<MeanBackward0>)\ntensor(0.3313, grad_fn=<MeanBackward0>)\ntensor(0.5604, grad_fn=<MeanBackward0>)\ntensor(0.5427, grad_fn=<MeanBackward0>)\ntensor(0.0240, grad_fn=<MeanBackward0>)\ntensor(0.1583, grad_fn=<MeanBackward0>)\ntensor(0.2641, grad_fn=<MeanBackward0>)\ntensor(8.0977e-05, grad_fn=<MeanBackward0>)\ntensor(0.0090, grad_fn=<MeanBackward0>)\ntensor(0.0540, grad_fn=<MeanBackward0>)\ntensor(6.0182e-06, grad_fn=<MeanBackward0>)\n34\ntensor(0.0018, grad_fn=<MeanBackward0>)\ntensor(0.0339, grad_fn=<MeanBackward0>)\ntensor(0.0197, grad_fn=<MeanBackward0>)\ntensor(0.0139, grad_fn=<MeanBackward0>)\ntensor(0.0757, grad_fn=<MeanBackward0>)\ntensor(0.0517, grad_fn=<MeanBackward0>)\ntensor(0.0063, grad_fn=<MeanBackward0>)\ntensor(0.0566, grad_fn=<MeanBackward0>)\ntensor(0.3178, grad_fn=<MeanBackward0>)\ntensor(0.0099, grad_fn=<MeanBackward0>)\ntensor(0.0419, grad_fn=<MeanBackward0>)\ntensor(0.0327, grad_fn=<MeanBackward0>)\ntensor(0.0028, grad_fn=<MeanBackward0>)\ntensor(0.0014, grad_fn=<MeanBackward0>)\ntensor(0.1591, grad_fn=<MeanBackward0>)\ntensor(0.0129, grad_fn=<MeanBackward0>)\ntensor(0.0309, grad_fn=<MeanBackward0>)\ntensor(0.0104, grad_fn=<MeanBackward0>)\ntensor(0.0861, grad_fn=<MeanBackward0>)\ntensor(0.1064, grad_fn=<MeanBackward0>)\ntensor(0.0188, grad_fn=<MeanBackward0>)\ntensor(0.0016, grad_fn=<MeanBackward0>)\ntensor(0.0693, grad_fn=<MeanBackward0>)\ntensor(0.1343, grad_fn=<MeanBackward0>)\ntensor(0.0411, grad_fn=<MeanBackward0>)\ntensor(0.0013, grad_fn=<MeanBackward0>)\ntensor(0.0034, grad_fn=<MeanBackward0>)\ntensor(0.6486, grad_fn=<MeanBackward0>)\ntensor(0.0553, grad_fn=<MeanBackward0>)\ntensor(0.0106, grad_fn=<MeanBackward0>)\ntensor(0.0134, grad_fn=<MeanBackward0>)\ntensor(0.0149, grad_fn=<MeanBackward0>)\n35\ntensor(0.0067, grad_fn=<MeanBackward0>)\ntensor(0.2506, grad_fn=<MeanBackward0>)\ntensor(0.3607, grad_fn=<MeanBackward0>)\ntensor(0.4284, grad_fn=<MeanBackward0>)\ntensor(0.1478, grad_fn=<MeanBackward0>)\ntensor(0.0090, grad_fn=<MeanBackward0>)\ntensor(0.0346, grad_fn=<MeanBackward0>)\ntensor(0.0301, grad_fn=<MeanBackward0>)\ntensor(0.5104, grad_fn=<MeanBackward0>)\ntensor(0.0495, grad_fn=<MeanBackward0>)\ntensor(0.2340, grad_fn=<MeanBackward0>)\ntensor(0.0573, grad_fn=<MeanBackward0>)\ntensor(0.0189, grad_fn=<MeanBackward0>)\ntensor(0.0167, grad_fn=<MeanBackward0>)\ntensor(0.1163, grad_fn=<MeanBackward0>)\ntensor(0.3034, grad_fn=<MeanBackward0>)\ntensor(0.1121, grad_fn=<MeanBackward0>)\ntensor(0.0296, grad_fn=<MeanBackward0>)\ntensor(0.0003, grad_fn=<MeanBackward0>)\ntensor(0.0089, grad_fn=<MeanBackward0>)\ntensor(0.0008, grad_fn=<MeanBackward0>)\ntensor(0.0393, grad_fn=<MeanBackward0>)\ntensor(0.0974, grad_fn=<MeanBackward0>)\ntensor(0.1536, grad_fn=<MeanBackward0>)\ntensor(0.0642, grad_fn=<MeanBackward0>)\ntensor(0.0006, grad_fn=<MeanBackward0>)\ntensor(0.3392, grad_fn=<MeanBackward0>)\ntensor(0.0405, grad_fn=<MeanBackward0>)\ntensor(0.0543, grad_fn=<MeanBackward0>)\ntensor(0.0113, grad_fn=<MeanBackward0>)\ntensor(0.0247, grad_fn=<MeanBackward0>)\ntensor(0.0073, grad_fn=<MeanBackward0>)\n36\ntensor(0.0032, grad_fn=<MeanBackward0>)\ntensor(0.0913, grad_fn=<MeanBackward0>)\ntensor(0.0363, grad_fn=<MeanBackward0>)\ntensor(0.0003, grad_fn=<MeanBackward0>)\ntensor(0.0294, grad_fn=<MeanBackward0>)\ntensor(0.0186, grad_fn=<MeanBackward0>)\ntensor(0.0232, grad_fn=<MeanBackward0>)\ntensor(0.0002, grad_fn=<MeanBackward0>)\ntensor(0.0033, grad_fn=<MeanBackward0>)\ntensor(0.0121, grad_fn=<MeanBackward0>)\ntensor(0.0025, grad_fn=<MeanBackward0>)\ntensor(0.0311, grad_fn=<MeanBackward0>)\ntensor(0.0452, grad_fn=<MeanBackward0>)\ntensor(0.0313, grad_fn=<MeanBackward0>)\ntensor(0.0037, grad_fn=<MeanBackward0>)\ntensor(0.1822, grad_fn=<MeanBackward0>)\ntensor(0.1701, grad_fn=<MeanBackward0>)\ntensor(0.1534, grad_fn=<MeanBackward0>)\ntensor(0.0240, grad_fn=<MeanBackward0>)\ntensor(0.0712, grad_fn=<MeanBackward0>)\ntensor(0.1958, grad_fn=<MeanBackward0>)\ntensor(0.0144, grad_fn=<MeanBackward0>)\ntensor(0.0001, grad_fn=<MeanBackward0>)\ntensor(0.1992, grad_fn=<MeanBackward0>)\ntensor(0.1032, grad_fn=<MeanBackward0>)\ntensor(0.0750, grad_fn=<MeanBackward0>)\ntensor(0.1108, grad_fn=<MeanBackward0>)\ntensor(0.0714, grad_fn=<MeanBackward0>)\ntensor(0.0081, grad_fn=<MeanBackward0>)\ntensor(0.0021, grad_fn=<MeanBackward0>)\ntensor(0.1853, grad_fn=<MeanBackward0>)\ntensor(0.2805, grad_fn=<MeanBackward0>)\n37\ntensor(0.0018, grad_fn=<MeanBackward0>)\ntensor(0.1332, grad_fn=<MeanBackward0>)\ntensor(5.7154e-06, grad_fn=<MeanBackward0>)\ntensor(0.2260, grad_fn=<MeanBackward0>)\ntensor(0.1530, grad_fn=<MeanBackward0>)\ntensor(0.1945, grad_fn=<MeanBackward0>)\ntensor(0.1318, grad_fn=<MeanBackward0>)\ntensor(0.1599, grad_fn=<MeanBackward0>)\ntensor(0.0452, grad_fn=<MeanBackward0>)\ntensor(5.0985e-06, grad_fn=<MeanBackward0>)\ntensor(0.0040, grad_fn=<MeanBackward0>)\ntensor(0.0032, grad_fn=<MeanBackward0>)\ntensor(0.0217, grad_fn=<MeanBackward0>)\ntensor(0.1123, grad_fn=<MeanBackward0>)\ntensor(0.0708, grad_fn=<MeanBackward0>)\ntensor(0.0120, grad_fn=<MeanBackward0>)\ntensor(0.0009, grad_fn=<MeanBackward0>)\ntensor(0.0110, grad_fn=<MeanBackward0>)\ntensor(0.0617, grad_fn=<MeanBackward0>)\ntensor(0.0026, grad_fn=<MeanBackward0>)\ntensor(0.0002, grad_fn=<MeanBackward0>)\ntensor(0.0025, grad_fn=<MeanBackward0>)\ntensor(0.0002, grad_fn=<MeanBackward0>)\ntensor(0.0030, grad_fn=<MeanBackward0>)\ntensor(0.0158, grad_fn=<MeanBackward0>)\ntensor(0.0010, grad_fn=<MeanBackward0>)\ntensor(0.0095, grad_fn=<MeanBackward0>)\ntensor(3.7859e-05, grad_fn=<MeanBackward0>)\ntensor(0.0214, grad_fn=<MeanBackward0>)\ntensor(0.0038, grad_fn=<MeanBackward0>)\ntensor(0.0221, grad_fn=<MeanBackward0>)\ntensor(0.0201, grad_fn=<MeanBackward0>)\n38\ntensor(2.9145e-06, grad_fn=<MeanBackward0>)\ntensor(0.0117, grad_fn=<MeanBackward0>)\ntensor(0.0419, grad_fn=<MeanBackward0>)\ntensor(0.0318, grad_fn=<MeanBackward0>)\ntensor(0.0003, grad_fn=<MeanBackward0>)\ntensor(0.0480, grad_fn=<MeanBackward0>)\ntensor(0.0014, grad_fn=<MeanBackward0>)\ntensor(0.0570, grad_fn=<MeanBackward0>)\ntensor(0.1199, grad_fn=<MeanBackward0>)\ntensor(0.0181, grad_fn=<MeanBackward0>)\ntensor(0.5167, grad_fn=<MeanBackward0>)\ntensor(0.3083, grad_fn=<MeanBackward0>)\ntensor(0.0568, grad_fn=<MeanBackward0>)\ntensor(0.0493, grad_fn=<MeanBackward0>)\ntensor(0.0993, grad_fn=<MeanBackward0>)\ntensor(4.2839e-05, grad_fn=<MeanBackward0>)\ntensor(0.0109, grad_fn=<MeanBackward0>)\ntensor(0.1333, grad_fn=<MeanBackward0>)\ntensor(0.2231, grad_fn=<MeanBackward0>)\ntensor(0.1655, grad_fn=<MeanBackward0>)\ntensor(0.0023, grad_fn=<MeanBackward0>)\ntensor(0.0786, grad_fn=<MeanBackward0>)\ntensor(0.0609, grad_fn=<MeanBackward0>)\ntensor(0.0345, grad_fn=<MeanBackward0>)\ntensor(0.0776, grad_fn=<MeanBackward0>)\ntensor(0.0690, grad_fn=<MeanBackward0>)\ntensor(0.0455, grad_fn=<MeanBackward0>)\ntensor(0.0673, grad_fn=<MeanBackward0>)\ntensor(0.1435, grad_fn=<MeanBackward0>)\ntensor(0.1174, grad_fn=<MeanBackward0>)\ntensor(0.0069, grad_fn=<MeanBackward0>)\ntensor(0.0356, grad_fn=<MeanBackward0>)\n39\ntensor(0.0044, grad_fn=<MeanBackward0>)\ntensor(0.0022, grad_fn=<MeanBackward0>)\ntensor(0.0113, grad_fn=<MeanBackward0>)\ntensor(0.3808, grad_fn=<MeanBackward0>)\ntensor(0.0131, grad_fn=<MeanBackward0>)\ntensor(0.0005, grad_fn=<MeanBackward0>)\ntensor(0.0679, grad_fn=<MeanBackward0>)\ntensor(0.0034, grad_fn=<MeanBackward0>)\ntensor(0.0040, grad_fn=<MeanBackward0>)\ntensor(0.1506, grad_fn=<MeanBackward0>)\ntensor(0.0296, grad_fn=<MeanBackward0>)\ntensor(0.0255, grad_fn=<MeanBackward0>)\ntensor(0.0180, grad_fn=<MeanBackward0>)\ntensor(0.0199, grad_fn=<MeanBackward0>)\ntensor(0.0070, grad_fn=<MeanBackward0>)\ntensor(0.1066, grad_fn=<MeanBackward0>)\ntensor(0.0786, grad_fn=<MeanBackward0>)\ntensor(0.0322, grad_fn=<MeanBackward0>)\ntensor(0.0791, grad_fn=<MeanBackward0>)\ntensor(0.0083, grad_fn=<MeanBackward0>)\ntensor(0.0167, grad_fn=<MeanBackward0>)\ntensor(0.0066, grad_fn=<MeanBackward0>)\ntensor(0.0015, grad_fn=<MeanBackward0>)\ntensor(0.0034, grad_fn=<MeanBackward0>)\ntensor(0.0183, grad_fn=<MeanBackward0>)\ntensor(0.0835, grad_fn=<MeanBackward0>)\ntensor(0.0010, grad_fn=<MeanBackward0>)\ntensor(0.0004, grad_fn=<MeanBackward0>)\ntensor(0.0024, grad_fn=<MeanBackward0>)\ntensor(0.0091, grad_fn=<MeanBackward0>)\ntensor(0.0964, grad_fn=<MeanBackward0>)\ntensor(0.0132, grad_fn=<MeanBackward0>)\n40\ntensor(0.0546, grad_fn=<MeanBackward0>)\ntensor(0.0010, grad_fn=<MeanBackward0>)\ntensor(0.0494, grad_fn=<MeanBackward0>)\ntensor(0.0238, grad_fn=<MeanBackward0>)\ntensor(0.0795, grad_fn=<MeanBackward0>)\ntensor(0.1139, grad_fn=<MeanBackward0>)\ntensor(0.0284, grad_fn=<MeanBackward0>)\ntensor(0.0011, grad_fn=<MeanBackward0>)\ntensor(0.0392, grad_fn=<MeanBackward0>)\ntensor(0.0147, grad_fn=<MeanBackward0>)\ntensor(0.0002, grad_fn=<MeanBackward0>)\ntensor(0.0093, grad_fn=<MeanBackward0>)\ntensor(0.0063, grad_fn=<MeanBackward0>)\ntensor(0.0318, grad_fn=<MeanBackward0>)\ntensor(0.0013, grad_fn=<MeanBackward0>)\ntensor(0.0001, grad_fn=<MeanBackward0>)\ntensor(0.0290, grad_fn=<MeanBackward0>)\ntensor(0.0024, grad_fn=<MeanBackward0>)\ntensor(0.1182, grad_fn=<MeanBackward0>)\ntensor(0.0359, grad_fn=<MeanBackward0>)\ntensor(0.0429, grad_fn=<MeanBackward0>)\ntensor(0.0006, grad_fn=<MeanBackward0>)\ntensor(0.0184, grad_fn=<MeanBackward0>)\ntensor(0.1108, grad_fn=<MeanBackward0>)\ntensor(0.1044, grad_fn=<MeanBackward0>)\ntensor(0.0276, grad_fn=<MeanBackward0>)\ntensor(0.0509, grad_fn=<MeanBackward0>)\ntensor(0.0018, grad_fn=<MeanBackward0>)\ntensor(0.0016, grad_fn=<MeanBackward0>)\ntensor(0.0627, grad_fn=<MeanBackward0>)\ntensor(0.0011, grad_fn=<MeanBackward0>)\ntensor(0.6019, grad_fn=<MeanBackward0>)\n41\ntensor(0.7249, grad_fn=<MeanBackward0>)\ntensor(0.2443, grad_fn=<MeanBackward0>)\ntensor(0.2037, grad_fn=<MeanBackward0>)\ntensor(0.1652, grad_fn=<MeanBackward0>)\ntensor(0.1189, grad_fn=<MeanBackward0>)\ntensor(0.2809, grad_fn=<MeanBackward0>)\ntensor(0.0217, grad_fn=<MeanBackward0>)\ntensor(0.0602, grad_fn=<MeanBackward0>)\ntensor(0.0205, grad_fn=<MeanBackward0>)\ntensor(0.0755, grad_fn=<MeanBackward0>)\ntensor(0.0111, grad_fn=<MeanBackward0>)\ntensor(0.0712, grad_fn=<MeanBackward0>)\ntensor(0.2615, grad_fn=<MeanBackward0>)\ntensor(0.2709, grad_fn=<MeanBackward0>)\ntensor(0.0016, grad_fn=<MeanBackward0>)\ntensor(0.0298, grad_fn=<MeanBackward0>)\ntensor(0.0085, grad_fn=<MeanBackward0>)\ntensor(0.2132, grad_fn=<MeanBackward0>)\ntensor(0.0224, grad_fn=<MeanBackward0>)\ntensor(0.0799, grad_fn=<MeanBackward0>)\ntensor(0.0444, grad_fn=<MeanBackward0>)\ntensor(0.0410, grad_fn=<MeanBackward0>)\ntensor(0.0581, grad_fn=<MeanBackward0>)\ntensor(0.1081, grad_fn=<MeanBackward0>)\ntensor(0.0027, grad_fn=<MeanBackward0>)\ntensor(0.4009, grad_fn=<MeanBackward0>)\ntensor(0.2364, grad_fn=<MeanBackward0>)\ntensor(0.1265, grad_fn=<MeanBackward0>)\ntensor(0.0813, grad_fn=<MeanBackward0>)\ntensor(0.0014, grad_fn=<MeanBackward0>)\ntensor(0.0852, grad_fn=<MeanBackward0>)\ntensor(0.0033, grad_fn=<MeanBackward0>)\n42\ntensor(0.0456, grad_fn=<MeanBackward0>)\ntensor(0.0439, grad_fn=<MeanBackward0>)\ntensor(0.1563, grad_fn=<MeanBackward0>)\ntensor(2.7941e-06, grad_fn=<MeanBackward0>)\ntensor(0.0007, grad_fn=<MeanBackward0>)\ntensor(0.1270, grad_fn=<MeanBackward0>)\ntensor(0.0617, grad_fn=<MeanBackward0>)\ntensor(0.0056, grad_fn=<MeanBackward0>)\ntensor(0.1592, grad_fn=<MeanBackward0>)\ntensor(0.0632, grad_fn=<MeanBackward0>)\ntensor(0.0057, grad_fn=<MeanBackward0>)\ntensor(0.0994, grad_fn=<MeanBackward0>)\ntensor(0.0001, grad_fn=<MeanBackward0>)\ntensor(0.0610, grad_fn=<MeanBackward0>)\ntensor(0.1507, grad_fn=<MeanBackward0>)\ntensor(0.0193, grad_fn=<MeanBackward0>)\ntensor(0.0609, grad_fn=<MeanBackward0>)\ntensor(0.0692, grad_fn=<MeanBackward0>)\ntensor(0.0405, grad_fn=<MeanBackward0>)\ntensor(0.0342, grad_fn=<MeanBackward0>)\ntensor(0.0650, grad_fn=<MeanBackward0>)\ntensor(0.0010, grad_fn=<MeanBackward0>)\ntensor(0.0003, grad_fn=<MeanBackward0>)\ntensor(0.0006, grad_fn=<MeanBackward0>)\ntensor(0.1105, grad_fn=<MeanBackward0>)\ntensor(0.0743, grad_fn=<MeanBackward0>)\ntensor(0.0047, grad_fn=<MeanBackward0>)\ntensor(0.0006, grad_fn=<MeanBackward0>)\ntensor(0.0655, grad_fn=<MeanBackward0>)\ntensor(0.0561, grad_fn=<MeanBackward0>)\ntensor(0.0116, grad_fn=<MeanBackward0>)\ntensor(0.0418, grad_fn=<MeanBackward0>)\n43\ntensor(0.0352, grad_fn=<MeanBackward0>)\ntensor(0.1786, grad_fn=<MeanBackward0>)\ntensor(0.0195, grad_fn=<MeanBackward0>)\ntensor(0.0128, grad_fn=<MeanBackward0>)\ntensor(0.1118, grad_fn=<MeanBackward0>)\ntensor(0.0223, grad_fn=<MeanBackward0>)\ntensor(0.0233, grad_fn=<MeanBackward0>)\ntensor(0.0193, grad_fn=<MeanBackward0>)\ntensor(0.0566, grad_fn=<MeanBackward0>)\ntensor(0.0352, grad_fn=<MeanBackward0>)\ntensor(0.0296, grad_fn=<MeanBackward0>)\ntensor(0.0939, grad_fn=<MeanBackward0>)\ntensor(0.0088, grad_fn=<MeanBackward0>)\ntensor(0.0070, grad_fn=<MeanBackward0>)\ntensor(0.7662, grad_fn=<MeanBackward0>)\ntensor(0.0910, grad_fn=<MeanBackward0>)\ntensor(0.0476, grad_fn=<MeanBackward0>)\ntensor(0.0443, grad_fn=<MeanBackward0>)\ntensor(0.0014, grad_fn=<MeanBackward0>)\ntensor(0.1437, grad_fn=<MeanBackward0>)\ntensor(3.8688e-05, grad_fn=<MeanBackward0>)\ntensor(0.0054, grad_fn=<MeanBackward0>)\ntensor(0.0035, grad_fn=<MeanBackward0>)\ntensor(0.0022, grad_fn=<MeanBackward0>)\ntensor(0.0251, grad_fn=<MeanBackward0>)\ntensor(0.0439, grad_fn=<MeanBackward0>)\ntensor(0.0009, grad_fn=<MeanBackward0>)\ntensor(0.0110, grad_fn=<MeanBackward0>)\ntensor(0.0039, grad_fn=<MeanBackward0>)\ntensor(0.0737, grad_fn=<MeanBackward0>)\ntensor(0.0085, grad_fn=<MeanBackward0>)\ntensor(0.1442, grad_fn=<MeanBackward0>)\n44\ntensor(0.0002, grad_fn=<MeanBackward0>)\ntensor(0.0013, grad_fn=<MeanBackward0>)\ntensor(0.0313, grad_fn=<MeanBackward0>)\ntensor(0.0087, grad_fn=<MeanBackward0>)\ntensor(0.0022, grad_fn=<MeanBackward0>)\ntensor(0.0006, grad_fn=<MeanBackward0>)\ntensor(0.0051, grad_fn=<MeanBackward0>)\ntensor(0.0031, grad_fn=<MeanBackward0>)\ntensor(0.1116, grad_fn=<MeanBackward0>)\ntensor(0.0963, grad_fn=<MeanBackward0>)\ntensor(0.0018, grad_fn=<MeanBackward0>)\ntensor(0.0712, grad_fn=<MeanBackward0>)\ntensor(0.1150, grad_fn=<MeanBackward0>)\ntensor(0.0105, grad_fn=<MeanBackward0>)\ntensor(0.0002, grad_fn=<MeanBackward0>)\ntensor(0.0156, grad_fn=<MeanBackward0>)\ntensor(0.0436, grad_fn=<MeanBackward0>)\ntensor(0.0098, grad_fn=<MeanBackward0>)\ntensor(0.0060, grad_fn=<MeanBackward0>)\ntensor(0.1005, grad_fn=<MeanBackward0>)\ntensor(0.0010, grad_fn=<MeanBackward0>)\ntensor(0.0414, grad_fn=<MeanBackward0>)\ntensor(0.0023, grad_fn=<MeanBackward0>)\ntensor(0.0083, grad_fn=<MeanBackward0>)\ntensor(0.0607, grad_fn=<MeanBackward0>)\ntensor(0.1324, grad_fn=<MeanBackward0>)\ntensor(0.0001, grad_fn=<MeanBackward0>)\ntensor(0.0026, grad_fn=<MeanBackward0>)\ntensor(0.0301, grad_fn=<MeanBackward0>)\ntensor(0.0209, grad_fn=<MeanBackward0>)\ntensor(0.0406, grad_fn=<MeanBackward0>)\ntensor(0.2336, grad_fn=<MeanBackward0>)\n45\ntensor(0.0569, grad_fn=<MeanBackward0>)\ntensor(0.0213, grad_fn=<MeanBackward0>)\ntensor(0.0068, grad_fn=<MeanBackward0>)\ntensor(0.0067, grad_fn=<MeanBackward0>)\ntensor(0.0454, grad_fn=<MeanBackward0>)\ntensor(0.0411, grad_fn=<MeanBackward0>)\ntensor(0.0054, grad_fn=<MeanBackward0>)\ntensor(0.0154, grad_fn=<MeanBackward0>)\ntensor(0.0122, grad_fn=<MeanBackward0>)\ntensor(0.0262, grad_fn=<MeanBackward0>)\ntensor(0.0431, grad_fn=<MeanBackward0>)\ntensor(0.0593, grad_fn=<MeanBackward0>)\ntensor(0.0073, grad_fn=<MeanBackward0>)\ntensor(0.0748, grad_fn=<MeanBackward0>)\ntensor(0.0543, grad_fn=<MeanBackward0>)\ntensor(0.0037, grad_fn=<MeanBackward0>)\ntensor(0.0097, grad_fn=<MeanBackward0>)\ntensor(0.0031, grad_fn=<MeanBackward0>)\ntensor(0.0209, grad_fn=<MeanBackward0>)\ntensor(0.0238, grad_fn=<MeanBackward0>)\ntensor(0.0008, grad_fn=<MeanBackward0>)\ntensor(0.0018, grad_fn=<MeanBackward0>)\ntensor(0.0240, grad_fn=<MeanBackward0>)\ntensor(0.3186, grad_fn=<MeanBackward0>)\ntensor(0.2043, grad_fn=<MeanBackward0>)\ntensor(0.0110, grad_fn=<MeanBackward0>)\ntensor(3.2466e-05, grad_fn=<MeanBackward0>)\ntensor(0.0060, grad_fn=<MeanBackward0>)\ntensor(0.0043, grad_fn=<MeanBackward0>)\ntensor(0.0268, grad_fn=<MeanBackward0>)\ntensor(0.0728, grad_fn=<MeanBackward0>)\ntensor(0.5239, grad_fn=<MeanBackward0>)\n46\ntensor(0.2155, grad_fn=<MeanBackward0>)\ntensor(0.0335, grad_fn=<MeanBackward0>)\ntensor(0.0570, grad_fn=<MeanBackward0>)\ntensor(0.3313, grad_fn=<MeanBackward0>)\ntensor(0.0005, grad_fn=<MeanBackward0>)\ntensor(0.2338, grad_fn=<MeanBackward0>)\ntensor(0.2514, grad_fn=<MeanBackward0>)\ntensor(0.0687, grad_fn=<MeanBackward0>)\ntensor(0.0065, grad_fn=<MeanBackward0>)\ntensor(0.0068, grad_fn=<MeanBackward0>)\ntensor(0.0123, grad_fn=<MeanBackward0>)\ntensor(0.0032, grad_fn=<MeanBackward0>)\ntensor(0.0002, grad_fn=<MeanBackward0>)\ntensor(0.0732, grad_fn=<MeanBackward0>)\ntensor(0.0011, grad_fn=<MeanBackward0>)\ntensor(0.0132, grad_fn=<MeanBackward0>)\ntensor(0.1230, grad_fn=<MeanBackward0>)\ntensor(0.0308, grad_fn=<MeanBackward0>)\ntensor(0.0051, grad_fn=<MeanBackward0>)\ntensor(0.0559, grad_fn=<MeanBackward0>)\ntensor(0.1568, grad_fn=<MeanBackward0>)\ntensor(0.0025, grad_fn=<MeanBackward0>)\ntensor(0.0049, grad_fn=<MeanBackward0>)\ntensor(0.0662, grad_fn=<MeanBackward0>)\ntensor(0.1584, grad_fn=<MeanBackward0>)\ntensor(0.0107, grad_fn=<MeanBackward0>)\ntensor(0.0104, grad_fn=<MeanBackward0>)\ntensor(0.0013, grad_fn=<MeanBackward0>)\ntensor(0.0736, grad_fn=<MeanBackward0>)\ntensor(0.0003, grad_fn=<MeanBackward0>)\ntensor(0.1466, grad_fn=<MeanBackward0>)\ntensor(0.2178, grad_fn=<MeanBackward0>)\n47\ntensor(0.0617, grad_fn=<MeanBackward0>)\ntensor(0.0798, grad_fn=<MeanBackward0>)\ntensor(0.0030, grad_fn=<MeanBackward0>)\ntensor(0.0441, grad_fn=<MeanBackward0>)\ntensor(0.0242, grad_fn=<MeanBackward0>)\ntensor(0.0480, grad_fn=<MeanBackward0>)\ntensor(0.0105, grad_fn=<MeanBackward0>)\ntensor(0.0358, grad_fn=<MeanBackward0>)\ntensor(0.0371, grad_fn=<MeanBackward0>)\ntensor(0.0004, grad_fn=<MeanBackward0>)\ntensor(0.0126, grad_fn=<MeanBackward0>)\ntensor(0.0079, grad_fn=<MeanBackward0>)\ntensor(0.0075, grad_fn=<MeanBackward0>)\ntensor(0.3030, grad_fn=<MeanBackward0>)\ntensor(0.0486, grad_fn=<MeanBackward0>)\ntensor(0.0171, grad_fn=<MeanBackward0>)\ntensor(0.0131, grad_fn=<MeanBackward0>)\ntensor(0.0302, grad_fn=<MeanBackward0>)\ntensor(0.0140, grad_fn=<MeanBackward0>)\ntensor(0.0182, grad_fn=<MeanBackward0>)\ntensor(0.0150, grad_fn=<MeanBackward0>)\ntensor(0.0076, grad_fn=<MeanBackward0>)\ntensor(4.6778e-05, grad_fn=<MeanBackward0>)\ntensor(0.1573, grad_fn=<MeanBackward0>)\ntensor(0.1483, grad_fn=<MeanBackward0>)\ntensor(0.0357, grad_fn=<MeanBackward0>)\ntensor(0.0048, grad_fn=<MeanBackward0>)\ntensor(0.0017, grad_fn=<MeanBackward0>)\ntensor(0.0666, grad_fn=<MeanBackward0>)\ntensor(0.0404, grad_fn=<MeanBackward0>)\ntensor(0.0336, grad_fn=<MeanBackward0>)\ntensor(0.0028, grad_fn=<MeanBackward0>)\n48\ntensor(0.1024, grad_fn=<MeanBackward0>)\ntensor(0.0789, grad_fn=<MeanBackward0>)\ntensor(0.0145, grad_fn=<MeanBackward0>)\ntensor(0.0009, grad_fn=<MeanBackward0>)\ntensor(0.0225, grad_fn=<MeanBackward0>)\ntensor(0.0557, grad_fn=<MeanBackward0>)\ntensor(0.0202, grad_fn=<MeanBackward0>)\ntensor(6.6783e-05, grad_fn=<MeanBackward0>)\ntensor(0.0015, grad_fn=<MeanBackward0>)\ntensor(0.0042, grad_fn=<MeanBackward0>)\ntensor(0.0008, grad_fn=<MeanBackward0>)\ntensor(0.0019, grad_fn=<MeanBackward0>)\ntensor(0.0073, grad_fn=<MeanBackward0>)\ntensor(0.0087, grad_fn=<MeanBackward0>)\ntensor(0.0031, grad_fn=<MeanBackward0>)\ntensor(0.0103, grad_fn=<MeanBackward0>)\ntensor(0.0088, grad_fn=<MeanBackward0>)\ntensor(0.0177, grad_fn=<MeanBackward0>)\ntensor(0.0094, grad_fn=<MeanBackward0>)\ntensor(0.0202, grad_fn=<MeanBackward0>)\ntensor(0.0219, grad_fn=<MeanBackward0>)\ntensor(0.0017, grad_fn=<MeanBackward0>)\ntensor(0.0186, grad_fn=<MeanBackward0>)\ntensor(0.0307, grad_fn=<MeanBackward0>)\ntensor(0.2172, grad_fn=<MeanBackward0>)\ntensor(2.5712e-07, grad_fn=<MeanBackward0>)\ntensor(0.0183, grad_fn=<MeanBackward0>)\ntensor(0.0023, grad_fn=<MeanBackward0>)\ntensor(0.0008, grad_fn=<MeanBackward0>)\ntensor(0.0076, grad_fn=<MeanBackward0>)\ntensor(0.0008, grad_fn=<MeanBackward0>)\ntensor(8.9351e-05, grad_fn=<MeanBackward0>)\n49\ntensor(0.0202, grad_fn=<MeanBackward0>)\ntensor(0.0043, grad_fn=<MeanBackward0>)\ntensor(0.0033, grad_fn=<MeanBackward0>)\ntensor(0.0055, grad_fn=<MeanBackward0>)\ntensor(0.0011, grad_fn=<MeanBackward0>)\ntensor(0.0011, grad_fn=<MeanBackward0>)\ntensor(0.0341, grad_fn=<MeanBackward0>)\ntensor(0.0634, grad_fn=<MeanBackward0>)\ntensor(0.0205, grad_fn=<MeanBackward0>)\ntensor(0.0031, grad_fn=<MeanBackward0>)\ntensor(0.0031, grad_fn=<MeanBackward0>)\ntensor(0.0013, grad_fn=<MeanBackward0>)\ntensor(0.0316, grad_fn=<MeanBackward0>)\ntensor(0.0264, grad_fn=<MeanBackward0>)\ntensor(0.0193, grad_fn=<MeanBackward0>)\ntensor(0.0242, grad_fn=<MeanBackward0>)\ntensor(0.0166, grad_fn=<MeanBackward0>)\ntensor(0.0068, grad_fn=<MeanBackward0>)\ntensor(0.0871, grad_fn=<MeanBackward0>)\ntensor(0.0565, grad_fn=<MeanBackward0>)\ntensor(0.0098, grad_fn=<MeanBackward0>)\ntensor(0.0003, grad_fn=<MeanBackward0>)\ntensor(0.0471, grad_fn=<MeanBackward0>)\ntensor(0.0167, grad_fn=<MeanBackward0>)\ntensor(0.0437, grad_fn=<MeanBackward0>)\ntensor(0.0339, grad_fn=<MeanBackward0>)\ntensor(0.0075, grad_fn=<MeanBackward0>)\ntensor(0.0617, grad_fn=<MeanBackward0>)\ntensor(0.1050, grad_fn=<MeanBackward0>)\ntensor(0.0646, grad_fn=<MeanBackward0>)\ntensor(0.0123, grad_fn=<MeanBackward0>)\ntensor(0.0381, grad_fn=<MeanBackward0>)\n50\ntensor(0.0020, grad_fn=<MeanBackward0>)\ntensor(0.0098, grad_fn=<MeanBackward0>)\ntensor(0.1580, grad_fn=<MeanBackward0>)\ntensor(0.0001, grad_fn=<MeanBackward0>)\ntensor(0.0113, grad_fn=<MeanBackward0>)\ntensor(0.0018, grad_fn=<MeanBackward0>)\ntensor(0.0561, grad_fn=<MeanBackward0>)\ntensor(0.0010, grad_fn=<MeanBackward0>)\ntensor(0.0632, grad_fn=<MeanBackward0>)\ntensor(0.0039, grad_fn=<MeanBackward0>)\ntensor(0.0040, grad_fn=<MeanBackward0>)\ntensor(0.0081, grad_fn=<MeanBackward0>)\ntensor(0.0069, grad_fn=<MeanBackward0>)\ntensor(0.1960, grad_fn=<MeanBackward0>)\ntensor(0.1020, grad_fn=<MeanBackward0>)\ntensor(0.0012, grad_fn=<MeanBackward0>)\ntensor(3.2122e-06, grad_fn=<MeanBackward0>)\ntensor(0.0526, grad_fn=<MeanBackward0>)\ntensor(0.0287, grad_fn=<MeanBackward0>)\ntensor(0.0036, grad_fn=<MeanBackward0>)\ntensor(0.0446, grad_fn=<MeanBackward0>)\ntensor(0.0224, grad_fn=<MeanBackward0>)\ntensor(0.0005, grad_fn=<MeanBackward0>)\ntensor(0.0300, grad_fn=<MeanBackward0>)\ntensor(0.0362, grad_fn=<MeanBackward0>)\ntensor(0.0067, grad_fn=<MeanBackward0>)\ntensor(0.0155, grad_fn=<MeanBackward0>)\ntensor(0.2269, grad_fn=<MeanBackward0>)\ntensor(0.0376, grad_fn=<MeanBackward0>)\ntensor(0.0477, grad_fn=<MeanBackward0>)\ntensor(0.0106, grad_fn=<MeanBackward0>)\ntensor(0.0367, grad_fn=<MeanBackward0>)\n51\ntensor(0.0016, grad_fn=<MeanBackward0>)\ntensor(0.0468, grad_fn=<MeanBackward0>)\ntensor(0.0001, grad_fn=<MeanBackward0>)\ntensor(0.0356, grad_fn=<MeanBackward0>)\ntensor(0.0088, grad_fn=<MeanBackward0>)\ntensor(0.0150, grad_fn=<MeanBackward0>)\ntensor(0.0482, grad_fn=<MeanBackward0>)\ntensor(0.0324, grad_fn=<MeanBackward0>)\ntensor(0.0029, grad_fn=<MeanBackward0>)\ntensor(0.1233, grad_fn=<MeanBackward0>)\ntensor(0.0002, grad_fn=<MeanBackward0>)\ntensor(0.0834, grad_fn=<MeanBackward0>)\ntensor(0.0733, grad_fn=<MeanBackward0>)\ntensor(1.6838e-05, grad_fn=<MeanBackward0>)\ntensor(0.0018, grad_fn=<MeanBackward0>)\ntensor(0.0059, grad_fn=<MeanBackward0>)\ntensor(0.0490, grad_fn=<MeanBackward0>)\ntensor(0.0059, grad_fn=<MeanBackward0>)\ntensor(0.0684, grad_fn=<MeanBackward0>)\ntensor(0.1041, grad_fn=<MeanBackward0>)\ntensor(0.0016, grad_fn=<MeanBackward0>)\ntensor(0.0031, grad_fn=<MeanBackward0>)\ntensor(0.0026, grad_fn=<MeanBackward0>)\ntensor(0.0188, grad_fn=<MeanBackward0>)\ntensor(0.0268, grad_fn=<MeanBackward0>)\ntensor(0.0112, grad_fn=<MeanBackward0>)\ntensor(0.0567, grad_fn=<MeanBackward0>)\ntensor(0.0329, grad_fn=<MeanBackward0>)\ntensor(0.0220, grad_fn=<MeanBackward0>)\ntensor(0.0018, grad_fn=<MeanBackward0>)\ntensor(0.0207, grad_fn=<MeanBackward0>)\ntensor(0.0390, grad_fn=<MeanBackward0>)\n52\ntensor(0.0253, grad_fn=<MeanBackward0>)\ntensor(0.0133, grad_fn=<MeanBackward0>)\ntensor(0.0033, grad_fn=<MeanBackward0>)\ntensor(0.0214, grad_fn=<MeanBackward0>)\ntensor(0.0035, grad_fn=<MeanBackward0>)\ntensor(0.0017, grad_fn=<MeanBackward0>)\ntensor(0.0717, grad_fn=<MeanBackward0>)\ntensor(0.0134, grad_fn=<MeanBackward0>)\ntensor(0.0024, grad_fn=<MeanBackward0>)\ntensor(0.0139, grad_fn=<MeanBackward0>)\ntensor(0.0002, grad_fn=<MeanBackward0>)\ntensor(0.1069, grad_fn=<MeanBackward0>)\ntensor(0.0685, grad_fn=<MeanBackward0>)\ntensor(0.0253, grad_fn=<MeanBackward0>)\ntensor(0.1710, grad_fn=<MeanBackward0>)\ntensor(0.0066, grad_fn=<MeanBackward0>)\ntensor(0.0047, grad_fn=<MeanBackward0>)\ntensor(0.0084, grad_fn=<MeanBackward0>)\ntensor(0.0201, grad_fn=<MeanBackward0>)\ntensor(0.0706, grad_fn=<MeanBackward0>)\ntensor(0.0001, grad_fn=<MeanBackward0>)\ntensor(0.0070, grad_fn=<MeanBackward0>)\ntensor(0.0035, grad_fn=<MeanBackward0>)\ntensor(0.0047, grad_fn=<MeanBackward0>)\ntensor(0.0107, grad_fn=<MeanBackward0>)\ntensor(0.0137, grad_fn=<MeanBackward0>)\ntensor(0.0450, grad_fn=<MeanBackward0>)\ntensor(0.0112, grad_fn=<MeanBackward0>)\ntensor(0.0028, grad_fn=<MeanBackward0>)\ntensor(0.0231, grad_fn=<MeanBackward0>)\ntensor(0.0329, grad_fn=<MeanBackward0>)\ntensor(0.0609, grad_fn=<MeanBackward0>)\n53\ntensor(0.0308, grad_fn=<MeanBackward0>)\ntensor(0.0095, grad_fn=<MeanBackward0>)\ntensor(0.0040, grad_fn=<MeanBackward0>)\ntensor(0.0239, grad_fn=<MeanBackward0>)\ntensor(0.0520, grad_fn=<MeanBackward0>)\ntensor(0.2788, grad_fn=<MeanBackward0>)\ntensor(0.2763, grad_fn=<MeanBackward0>)\ntensor(0.0112, grad_fn=<MeanBackward0>)\ntensor(0.0102, grad_fn=<MeanBackward0>)\ntensor(0.0380, grad_fn=<MeanBackward0>)\ntensor(0.0209, grad_fn=<MeanBackward0>)\ntensor(0.0065, grad_fn=<MeanBackward0>)\ntensor(0.0012, grad_fn=<MeanBackward0>)\ntensor(0.0009, grad_fn=<MeanBackward0>)\ntensor(0.0081, grad_fn=<MeanBackward0>)\ntensor(0.0022, grad_fn=<MeanBackward0>)\ntensor(3.7568e-06, grad_fn=<MeanBackward0>)\ntensor(0.0076, grad_fn=<MeanBackward0>)\ntensor(9.7994e-05, grad_fn=<MeanBackward0>)\ntensor(0.0196, grad_fn=<MeanBackward0>)\ntensor(0.0444, grad_fn=<MeanBackward0>)\ntensor(0.0314, grad_fn=<MeanBackward0>)\ntensor(0.0027, grad_fn=<MeanBackward0>)\ntensor(0.0064, grad_fn=<MeanBackward0>)\ntensor(0.0016, grad_fn=<MeanBackward0>)\ntensor(0.0141, grad_fn=<MeanBackward0>)\ntensor(0.0118, grad_fn=<MeanBackward0>)\ntensor(0.0281, grad_fn=<MeanBackward0>)\ntensor(0.0193, grad_fn=<MeanBackward0>)\ntensor(0.0082, grad_fn=<MeanBackward0>)\ntensor(0.0234, grad_fn=<MeanBackward0>)\ntensor(0.0229, grad_fn=<MeanBackward0>)\n54\ntensor(0.0189, grad_fn=<MeanBackward0>)\ntensor(0.0073, grad_fn=<MeanBackward0>)\ntensor(0.0061, grad_fn=<MeanBackward0>)\ntensor(0.0012, grad_fn=<MeanBackward0>)\ntensor(0.0033, grad_fn=<MeanBackward0>)\ntensor(0.0195, grad_fn=<MeanBackward0>)\ntensor(0.0278, grad_fn=<MeanBackward0>)\ntensor(0.0010, grad_fn=<MeanBackward0>)\ntensor(0.0271, grad_fn=<MeanBackward0>)\ntensor(0.0542, grad_fn=<MeanBackward0>)\ntensor(0.0190, grad_fn=<MeanBackward0>)\ntensor(0.0005, grad_fn=<MeanBackward0>)\ntensor(0.0019, grad_fn=<MeanBackward0>)\ntensor(0.0034, grad_fn=<MeanBackward0>)\ntensor(0.0435, grad_fn=<MeanBackward0>)\ntensor(0.0225, grad_fn=<MeanBackward0>)\ntensor(0.0067, grad_fn=<MeanBackward0>)\ntensor(0.0001, grad_fn=<MeanBackward0>)\ntensor(0.0001, grad_fn=<MeanBackward0>)\ntensor(0.0723, grad_fn=<MeanBackward0>)\ntensor(3.3851e-05, grad_fn=<MeanBackward0>)\ntensor(0.0042, grad_fn=<MeanBackward0>)\ntensor(0.0011, grad_fn=<MeanBackward0>)\ntensor(0.0302, grad_fn=<MeanBackward0>)\ntensor(0.0005, grad_fn=<MeanBackward0>)\ntensor(0.0020, grad_fn=<MeanBackward0>)\ntensor(0.0004, grad_fn=<MeanBackward0>)\ntensor(0.0035, grad_fn=<MeanBackward0>)\ntensor(0.0019, grad_fn=<MeanBackward0>)\ntensor(0.0021, grad_fn=<MeanBackward0>)\ntensor(0.0027, grad_fn=<MeanBackward0>)\ntensor(0.0008, grad_fn=<MeanBackward0>)\n55\ntensor(0.0047, grad_fn=<MeanBackward0>)\ntensor(0.0029, grad_fn=<MeanBackward0>)\ntensor(0.0022, grad_fn=<MeanBackward0>)\ntensor(0.0029, grad_fn=<MeanBackward0>)\ntensor(0.0012, grad_fn=<MeanBackward0>)\ntensor(0.0020, grad_fn=<MeanBackward0>)\ntensor(0.0011, grad_fn=<MeanBackward0>)\ntensor(0.0555, grad_fn=<MeanBackward0>)\ntensor(0.0004, grad_fn=<MeanBackward0>)\ntensor(0.0006, grad_fn=<MeanBackward0>)\ntensor(0.0020, grad_fn=<MeanBackward0>)\ntensor(0.0023, grad_fn=<MeanBackward0>)\ntensor(0.0047, grad_fn=<MeanBackward0>)\ntensor(0.0011, grad_fn=<MeanBackward0>)\ntensor(0.0012, grad_fn=<MeanBackward0>)\ntensor(0.0021, grad_fn=<MeanBackward0>)\ntensor(0.0003, grad_fn=<MeanBackward0>)\ntensor(0.0027, grad_fn=<MeanBackward0>)\ntensor(0.0002, grad_fn=<MeanBackward0>)\ntensor(6.4083e-05, grad_fn=<MeanBackward0>)\ntensor(0.0127, grad_fn=<MeanBackward0>)\ntensor(0.0006, grad_fn=<MeanBackward0>)\ntensor(0.0009, grad_fn=<MeanBackward0>)\ntensor(0.0181, grad_fn=<MeanBackward0>)\ntensor(0.0084, grad_fn=<MeanBackward0>)\ntensor(0.0045, grad_fn=<MeanBackward0>)\ntensor(0.0015, grad_fn=<MeanBackward0>)\ntensor(0.0011, grad_fn=<MeanBackward0>)\ntensor(3.0085e-07, grad_fn=<MeanBackward0>)\ntensor(0.0722, grad_fn=<MeanBackward0>)\ntensor(0.0061, grad_fn=<MeanBackward0>)\ntensor(0.0260, grad_fn=<MeanBackward0>)\n56\ntensor(0.0081, grad_fn=<MeanBackward0>)\ntensor(0.0310, grad_fn=<MeanBackward0>)\ntensor(0.0703, grad_fn=<MeanBackward0>)\ntensor(0.1048, grad_fn=<MeanBackward0>)\ntensor(0.0047, grad_fn=<MeanBackward0>)\ntensor(0.0159, grad_fn=<MeanBackward0>)\ntensor(0.0043, grad_fn=<MeanBackward0>)\ntensor(0.0019, grad_fn=<MeanBackward0>)\ntensor(0.0102, grad_fn=<MeanBackward0>)\ntensor(0.0159, grad_fn=<MeanBackward0>)\ntensor(0.0175, grad_fn=<MeanBackward0>)\ntensor(4.9194e-05, grad_fn=<MeanBackward0>)\ntensor(0.0122, grad_fn=<MeanBackward0>)\ntensor(0.0342, grad_fn=<MeanBackward0>)\ntensor(0.0643, grad_fn=<MeanBackward0>)\ntensor(0.0397, grad_fn=<MeanBackward0>)\ntensor(0.0078, grad_fn=<MeanBackward0>)\ntensor(0.0072, grad_fn=<MeanBackward0>)\ntensor(0.0076, grad_fn=<MeanBackward0>)\ntensor(0.0030, grad_fn=<MeanBackward0>)\ntensor(0.1116, grad_fn=<MeanBackward0>)\ntensor(0.0870, grad_fn=<MeanBackward0>)\ntensor(0.0411, grad_fn=<MeanBackward0>)\ntensor(3.7493e-05, grad_fn=<MeanBackward0>)\ntensor(0.0077, grad_fn=<MeanBackward0>)\ntensor(0.0102, grad_fn=<MeanBackward0>)\ntensor(0.0007, grad_fn=<MeanBackward0>)\ntensor(0.0826, grad_fn=<MeanBackward0>)\ntensor(0.1028, grad_fn=<MeanBackward0>)\ntensor(0.0003, grad_fn=<MeanBackward0>)\ntensor(0.0005, grad_fn=<MeanBackward0>)\ntensor(0.0016, grad_fn=<MeanBackward0>)\n57\ntensor(1.7934e-06, grad_fn=<MeanBackward0>)\ntensor(0.1312, grad_fn=<MeanBackward0>)\ntensor(0.1938, grad_fn=<MeanBackward0>)\ntensor(0.0091, grad_fn=<MeanBackward0>)\ntensor(0.0049, grad_fn=<MeanBackward0>)\ntensor(0.0020, grad_fn=<MeanBackward0>)\ntensor(0.0114, grad_fn=<MeanBackward0>)\ntensor(0.0094, grad_fn=<MeanBackward0>)\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1077-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1076-ac2fb38e4784>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mcur_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1075-c9567c827b85>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1075-c9567c827b85>\u001b[0m in \u001b[0;36m_train_critic\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0moptim_critic_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim_critic_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimize_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1075-c9567c827b85>\u001b[0m in \u001b[0;36moptimize_critic\u001b[0;34m(self, input_state, reward)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m#self.critic_model.action_input.retain_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;31m#loss = loss.double()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;31m#print(\"Fuck: \", self.critic_model.action_input.grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/euclid/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/euclid/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'output_dimension'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1070-a42b2d3d637a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mn_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mactor_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcritic_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'output_dimension'"
     ]
    }
   ],
   "source": [
    "# DEFINE TRANSITION OPERATIONS\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = 3\n",
    "\n",
    "actor_net = Actor(n_actions).to(device)\n",
    "\n",
    "critic_net = Critic(n_actions).to(device)\n",
    "\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# convert to double?\n",
    "policy_net = DQN(n_actions).float()\n",
    "target_net = DQN(n_actions).float()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            \"\"\"\n",
    "            best_action = policy_net(state)\n",
    "            best_action = best_action.max(1)\n",
    "            best_action = best_action[1]\n",
    "            best_action.view(1, 1)\n",
    "            \"\"\"\n",
    "            return  policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    \"\"\"\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    #print(batch)\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_batch = state_batch.view(BATCH_SIZE, 6)\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    non_final_next_states = non_final_next_states.view(non_final_next_states.shape[0] // 6, 6)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    state_action_values = state_action_values.double()\n",
    "    expected_state_action_values = expected_state_action_values.double()\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss.double()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1072-d287760b08a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Initialize the environment and state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    state = env.get_state()\n",
    "    for t in count():\n",
    "        print(t)\n",
    "        #print(state)\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        reward, done = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        if not done:\n",
    "            next_state = env.get_state()\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "#env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1073-8740b4d448e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "test_states = []\n",
    "test_actions = []\n",
    "\n",
    "env.reset()\n",
    "state = env.get_state()\n",
    "for t in count():\n",
    "    print(t)\n",
    "    #print(state)\n",
    "    # Select and perform an action\n",
    "    action = select_action(state)\n",
    "    reward, done = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "\n",
    "    test_states.append(state)\n",
    "    test_actions.append(action)\n",
    "\n",
    "    # Observe new state\n",
    "    if not done:\n",
    "        next_state = env.get_state()\n",
    "    else:\n",
    "        next_state = None\n",
    "\n",
    "\n",
    "    # Move to the next state\n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        episode_durations.append(t + 1)\n",
    "        plot_durations()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n<link rel=\"stylesheet\"\nhref=\"https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/\ncss/font-awesome.min.css\">\n<script language=\"javascript\">\n  function isInternetExplorer() {\n    ua = navigator.userAgent;\n    /* MSIE used to detect old browsers and Trident used to newer ones*/\n    return ua.indexOf(\"MSIE \") > -1 || ua.indexOf(\"Trident/\") > -1;\n  }\n\n  /* Define the Animation class */\n  function Animation(frames, img_id, slider_id, interval, loop_select_id){\n    this.img_id = img_id;\n    this.slider_id = slider_id;\n    this.loop_select_id = loop_select_id;\n    this.interval = interval;\n    this.current_frame = 0;\n    this.direction = 0;\n    this.timer = null;\n    this.frames = new Array(frames.length);\n\n    for (var i=0; i<frames.length; i++)\n    {\n     this.frames[i] = new Image();\n     this.frames[i].src = frames[i];\n    }\n    var slider = document.getElementById(this.slider_id);\n    slider.max = this.frames.length - 1;\n    if (isInternetExplorer()) {\n        // switch from oninput to onchange because IE <= 11 does not conform\n        // with W3C specification. It ignores oninput and onchange behaves\n        // like oninput. In contrast, Mircosoft Edge behaves correctly.\n        slider.setAttribute('onchange', slider.getAttribute('oninput'));\n        slider.setAttribute('oninput', null);\n    }\n    this.set_frame(this.current_frame);\n  }\n\n  Animation.prototype.get_loop_state = function(){\n    var button_group = document[this.loop_select_id].state;\n    for (var i = 0; i < button_group.length; i++) {\n        var button = button_group[i];\n        if (button.checked) {\n            return button.value;\n        }\n    }\n    return undefined;\n  }\n\n  Animation.prototype.set_frame = function(frame){\n    this.current_frame = frame;\n    document.getElementById(this.img_id).src =\n            this.frames[this.current_frame].src;\n    document.getElementById(this.slider_id).value = this.current_frame;\n  }\n\n  Animation.prototype.next_frame = function()\n  {\n    this.set_frame(Math.min(this.frames.length - 1, this.current_frame + 1));\n  }\n\n  Animation.prototype.previous_frame = function()\n  {\n    this.set_frame(Math.max(0, this.current_frame - 1));\n  }\n\n  Animation.prototype.first_frame = function()\n  {\n    this.set_frame(0);\n  }\n\n  Animation.prototype.last_frame = function()\n  {\n    this.set_frame(this.frames.length - 1);\n  }\n\n  Animation.prototype.slower = function()\n  {\n    this.interval /= 0.7;\n    if(this.direction > 0){this.play_animation();}\n    else if(this.direction < 0){this.reverse_animation();}\n  }\n\n  Animation.prototype.faster = function()\n  {\n    this.interval *= 0.7;\n    if(this.direction > 0){this.play_animation();}\n    else if(this.direction < 0){this.reverse_animation();}\n  }\n\n  Animation.prototype.anim_step_forward = function()\n  {\n    this.current_frame += 1;\n    if(this.current_frame < this.frames.length){\n      this.set_frame(this.current_frame);\n    }else{\n      var loop_state = this.get_loop_state();\n      if(loop_state == \"loop\"){\n        this.first_frame();\n      }else if(loop_state == \"reflect\"){\n        this.last_frame();\n        this.reverse_animation();\n      }else{\n        this.pause_animation();\n        this.last_frame();\n      }\n    }\n  }\n\n  Animation.prototype.anim_step_reverse = function()\n  {\n    this.current_frame -= 1;\n    if(this.current_frame >= 0){\n      this.set_frame(this.current_frame);\n    }else{\n      var loop_state = this.get_loop_state();\n      if(loop_state == \"loop\"){\n        this.last_frame();\n      }else if(loop_state == \"reflect\"){\n        this.first_frame();\n        this.play_animation();\n      }else{\n        this.pause_animation();\n        this.first_frame();\n      }\n    }\n  }\n\n  Animation.prototype.pause_animation = function()\n  {\n    this.direction = 0;\n    if (this.timer){\n      clearInterval(this.timer);\n      this.timer = null;\n    }\n  }\n\n  Animation.prototype.play_animation = function()\n  {\n    this.pause_animation();\n    this.direction = 1;\n    var t = this;\n    if (!this.timer) this.timer = setInterval(function() {\n        t.anim_step_forward();\n    }, this.interval);\n  }\n\n  Animation.prototype.reverse_animation = function()\n  {\n    this.pause_animation();\n    this.direction = -1;\n    var t = this;\n    if (!this.timer) this.timer = setInterval(function() {\n        t.anim_step_reverse();\n    }, this.interval);\n  }\n</script>\n\n<style>\n.animation {\n    display: inline-block;\n    text-align: center;\n}\ninput[type=range].anim-slider {\n    width: 374px;\n    margin-left: auto;\n    margin-right: auto;\n}\n.anim-buttons {\n    margin: 8px 0px;\n}\n.anim-buttons button {\n    padding: 0;\n    width: 36px;\n}\n.anim-state label {\n    margin-right: 8px;\n}\n.anim-state input {\n    margin: 0;\n    vertical-align: middle;\n}\n</style>\n\n<div class=\"animation\">\n  <img id=\"_anim_imgc87190c1fcc746f69d06b68fea7cfb16\">\n  <div class=\"anim-controls\">\n    <input id=\"_anim_sliderc87190c1fcc746f69d06b68fea7cfb16\" type=\"range\" class=\"anim-slider\"\n           name=\"points\" min=\"0\" max=\"1\" step=\"1\" value=\"0\"\n           oninput=\"animc87190c1fcc746f69d06b68fea7cfb16.set_frame(parseInt(this.value));\"></input>\n    <div class=\"anim-buttons\">\n      <button onclick=\"animc87190c1fcc746f69d06b68fea7cfb16.slower()\"><i class=\"fa fa-minus\"></i></button>\n      <button onclick=\"animc87190c1fcc746f69d06b68fea7cfb16.first_frame()\"><i class=\"fa fa-fast-backward\">\n          </i></button>\n      <button onclick=\"animc87190c1fcc746f69d06b68fea7cfb16.previous_frame()\">\n          <i class=\"fa fa-step-backward\"></i></button>\n      <button onclick=\"animc87190c1fcc746f69d06b68fea7cfb16.reverse_animation()\">\n          <i class=\"fa fa-play fa-flip-horizontal\"></i></button>\n      <button onclick=\"animc87190c1fcc746f69d06b68fea7cfb16.pause_animation()\"><i class=\"fa fa-pause\">\n          </i></button>\n      <button onclick=\"animc87190c1fcc746f69d06b68fea7cfb16.play_animation()\"><i class=\"fa fa-play\"></i>\n          </button>\n      <button onclick=\"animc87190c1fcc746f69d06b68fea7cfb16.next_frame()\"><i class=\"fa fa-step-forward\">\n          </i></button>\n      <button onclick=\"animc87190c1fcc746f69d06b68fea7cfb16.last_frame()\"><i class=\"fa fa-fast-forward\">\n          </i></button>\n      <button onclick=\"animc87190c1fcc746f69d06b68fea7cfb16.faster()\"><i class=\"fa fa-plus\"></i></button>\n    </div>\n    <form action=\"#n\" name=\"_anim_loop_selectc87190c1fcc746f69d06b68fea7cfb16\" class=\"anim-state\">\n      <input type=\"radio\" name=\"state\" value=\"once\" id=\"_anim_radio1_c87190c1fcc746f69d06b68fea7cfb16\"\n             >\n      <label for=\"_anim_radio1_c87190c1fcc746f69d06b68fea7cfb16\">Once</label>\n      <input type=\"radio\" name=\"state\" value=\"loop\" id=\"_anim_radio2_c87190c1fcc746f69d06b68fea7cfb16\"\n             checked>\n      <label for=\"_anim_radio2_c87190c1fcc746f69d06b68fea7cfb16\">Loop</label>\n      <input type=\"radio\" name=\"state\" value=\"reflect\" id=\"_anim_radio3_c87190c1fcc746f69d06b68fea7cfb16\"\n             >\n      <label for=\"_anim_radio3_c87190c1fcc746f69d06b68fea7cfb16\">Reflect</label>\n    </form>\n  </div>\n</div>\n\n\n<script language=\"javascript\">\n  /* Instantiate the Animation class. */\n  /* The IDs given should match those used in the template above. */\n  (function() {\n    var img_id = \"_anim_imgc87190c1fcc746f69d06b68fea7cfb16\";\n    var slider_id = \"_anim_sliderc87190c1fcc746f69d06b68fea7cfb16\";\n    var loop_select_id = \"_anim_loop_selectc87190c1fcc746f69d06b68fea7cfb16\";\n    var frames = new Array(0);\n    \n\n\n    /* set a timeout to make sure all the above elements are created before\n       the object is initialized. */\n    setTimeout(function() {\n        animc87190c1fcc746f69d06b68fea7cfb16 = new Animation(frames, img_id, slider_id, 299.0,\n                                 loop_select_id);\n    }, 0);\n  })()\n</script>\n"
     },
     "metadata": {},
     "execution_count": 1074
    }
   ],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "measurements = np.array(test_actions) == 8\n",
    "#=========================================\n",
    "# Animate Images\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ims = []\n",
    "for i in range(len(test_states)):\n",
    "    x = np.array(test_states[i])[2:5:2]\n",
    "    y = np.array(test_states[i])[3:6:2]\n",
    "    if (measurements[i]):\n",
    "        ims.append([plt.scatter(x, y, animated=True)])\n",
    "        im, = plt.plot(x, y, 'ro-', animated=True)\n",
    "    else:\n",
    "        im = plt.scatter(x, y, animated=True)\n",
    "    ims.append([im])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=300, blit=True, repeat_delay=1000)\n",
    "plt.close()\n",
    "\n",
    "# Show the animation\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('euclid': conda)",
   "language": "python",
   "name": "python38364biteuclidconda0c4f3e6856524f32859d293db3eeb7e9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}