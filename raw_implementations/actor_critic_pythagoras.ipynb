{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from itertools import count\n",
    "from collections import namedtuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "\"\"\"\n",
    "Actions:\n",
    "    -Draw vertex\n",
    "    -Measure distance\n",
    "    -Measure angle\n",
    "\"\"\"\n",
    "# DEFINE ENVIRONMENT\n",
    "class GeometryEnvironment():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def place_vertex(self, bead_index, vertex_index):\n",
    "        self.bead_set[bead_index] = vertex_index\n",
    "\n",
    "    def measure_distance(self, u, v):\n",
    "        return euclidean(u, v)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.bead_set = np.array([(randint(low=1, high=100), randint(low=1, high=100)) for i in range(2)])\n",
    "        self.defining_state = (randint(low=1, high=100), randint(low=1, high=100))\n",
    "        self.answer = 0\n",
    "        self.correct_answer = np.sqrt(self.defining_state[0]**2 + self.defining_state[1]**2)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # if distance should be measured\n",
    "        #print(\"beads at start of step\", self.bead_set)\n",
    "        # convert tensor to format which can be manipulated\n",
    "        if (action.requires_grad):\n",
    "            action = action.clone().detach().numpy().T\n",
    "        action_index = action[0]\n",
    "        action_vertex = (action[1], action[2])\n",
    "        if (action_index < 30):\n",
    "            self.answer = self.measure_distance(self.bead_set[0], self.bead_set[1])\n",
    "        else:\n",
    "            action_index = 0 if action_index < 60 else 1\n",
    "            self.place_vertex(action_index, action_vertex)\n",
    "        done = np.abs(self.answer - self.correct_answer) < 5\n",
    "        reward = 10.0 if done else 0.0 #1 / (np.abs(self.answer - self.correct_answer)**2)\n",
    "        #print(\"beads at end of step\", self.bead_set)\n",
    "        #print(\"distance: \", self.measure_distance(self.bead_set[0], self.bead_set[1]))\n",
    "        return self.get_state(), reward, done, -1\n",
    "    \n",
    "    def get_state(self):\n",
    "        a = np.array(self.defining_state)\n",
    "        b = np.array(self.bead_set[0])\n",
    "        c = np.array(self.bead_set[1])\n",
    "        nump = np.concatenate((a, b, c))\n",
    "        t_state = torch.tensor(nump, dtype=torch.long)\n",
    "        t_state = t_state.float().flatten()\n",
    "        t_state.requires_grad = True\n",
    "        return t_state\n",
    "    \n",
    "    def render(self):\n",
    "        #print(\"Answer: \", self.answer)\n",
    "        vertices = np.array([self.vertex_set[(self.bead_set[0])], self.vertex_set[(self.bead_set[1])]])\n",
    "        #print(vertices)\n",
    "        x, y = vertices.T\n",
    "        plt.scatter(x, y)\n",
    "        plt.show()\n",
    "    \n",
    "    def sample_action_space(self):\n",
    "        action_type = float(random.randint(0, 100))\n",
    "        return torch.tensor([action_type, float(randint(low=1, high=100)), float(randint(low=1, high=100))], requires_grad=True)\n",
    "    \n",
    "    def get_distances(self):\n",
    "        # self.answer,\n",
    "        return self.measure_distance(self.bead_set[0], self.bead_set[1]), self.correct_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLAY MEMORY DEFINITION\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems:\n",
    "\n",
    "-Need to provide float args to vertex position (x and y)\n",
    "\n",
    "-Need to provide index argument to vertex position\n",
    "\n",
    "-Need to provide index arguments to distance measurement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE NETWORK\n",
    "# TODO: replace CNN with linear learner\n",
    "\n",
    "def print_with_extreme_prejudice(num):\n",
    "    print(\"Fuck you you fucking piece of shit\", num)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dimension, output_dimension):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dimension, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.head = nn.Linear(32, output_dimension)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization\n",
    "    # Returns a distribution over the possible actions tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        self.out = self.head(x.view(1, x.size(0)))[0]\n",
    "        return  self.out # self.head(x.view(x.size(0), -1))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dimension, output_dimension):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(input_dimension, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.head = nn.Linear(32, output_dimension)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.state_input = torch.randn((1, 6), requires_grad=True)\n",
    "\n",
    "        self.state1 = nn.Linear(6, 32)  # 6*6 from image dimension\n",
    "        self.state2 = nn.Linear(32, 64)\n",
    "\n",
    "        self.action_input = torch.randn((1, 3), requires_grad=True)\n",
    "        self.action_input.retain_grad()\n",
    "        self.action1 = nn.Linear(3, 32)  # 6*6 from image dimension\n",
    "\n",
    "        self.merged1 = nn.Linear(96, 32)\n",
    "\n",
    "        self.head = nn.Linear(32, output_dimension)\n",
    "        self.out = torch.randn((1), requires_grad=True)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization\n",
    "    # Returns a distribution over the possible actions tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        self.action_input.register_hook(print_with_extreme_prejudice)\n",
    "        x = x.float()\n",
    "        #print(x.shape)\n",
    "        self.state_input = x[0:6].clone()\n",
    "        x_state = F.relu(self.state1(self.state_input))\n",
    "        x_state = F.relu(self.state2(x_state))\n",
    "\n",
    "        self.action_input = x[6:10].clone()\n",
    "        x_action = F.relu(self.action1(self.action_input))\n",
    "\n",
    "        x = torch.cat((x_state, x_action))\n",
    "        x = F.relu(self.merged1(x))\n",
    "        self.out = self.head(x.view(-1, x.size(0))) # self.head(x.view(x.size(0), -1))\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "def clone_tensors(item_list):\n",
    "    new_items = []\n",
    "    for item in item_list:\n",
    "        new_items.append(item.clone() if torch.is_tensor(item) else item)\n",
    "    return new_items\n",
    "\n",
    "class ActorCritic:\n",
    "    def __init__(self, env):\n",
    "        self.env  = env\n",
    "\n",
    "        self.action_dimension = 3\n",
    "        self.environment_dimension = 6     \n",
    "        \n",
    "        self.learning_rate = 0.01 #0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 50 #.995\n",
    "        self.gamma = 0.9 #.95\n",
    "        self.tau   = .125\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # ===================================================================== #\n",
    "        #                               Actor Model                             #\n",
    "        # Chain rule: find the gradient of changing the actor network params in #\n",
    "        # getting closest to the final value network predictions, i.e. de/dA    #\n",
    "        # Calculate de/dA as = de/dC * dC/dA, where e is error, C critic, A act #\n",
    "        # ===================================================================== #\n",
    "\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        self.actor_model = Actor(self.environment_dimension, self.action_dimension).float()\n",
    "        self.target_actor_model = Actor(self.environment_dimension, self.action_dimension).float()\n",
    "\n",
    "        self.target_actor_model.load_state_dict(self.actor_model.state_dict())\n",
    "        self.target_actor_model.eval()\n",
    "\n",
    "        #self.actor_optimizer = optim.Adam(self.actor_model.parameters())\n",
    "        self.actor_optimizer = optim.RMSprop(self.actor_model.parameters())\n",
    "        \n",
    "        #print(self.actor_model.state_dict())\n",
    "        \"\"\"\n",
    "        self.actor_critic_grad = tf.placeholder(tf.float32, \n",
    "            [None, self.env.action_space.shape[0]]) # where we will feed de/dC (from critic)\n",
    "        \n",
    "        actor_model_weights = self.actor_model.trainable_weights\n",
    "        self.actor_grads = tf.gradients(self.actor_model.output, \n",
    "            actor_model_weights, -self.actor_critic_grad) # dC/dA (from actor)\n",
    "        \n",
    "        grads = zip(self.actor_grads, actor_model_weights)\n",
    "\n",
    "        \n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)\n",
    "         optimizer = optim.Adam(policy_net.parameters())\n",
    "         \"\"\"\n",
    "\n",
    "        # ===================================================================== #\n",
    "        #                              Critic Model                             #\n",
    "        # ===================================================================== #        \n",
    "\n",
    "        self.critic_model = Critic(self.action_dimension + self.environment_dimension, 1).float()\n",
    "        self.target_critic_model = Critic(self.action_dimension + self.environment_dimension, 1).float()\n",
    "\n",
    "        self.target_critic_model.load_state_dict(self.critic_model.state_dict())\n",
    "        self.target_critic_model.eval()\n",
    "\n",
    "        #self.critic_grads =  torch.autograd.grad(self.critic_model.head, self.critic_model.action_input)\n",
    "\n",
    "        #self.critic_optimizer = optim.Adam(self.critic_model.parameters())\n",
    "        self.critic_optimizer = optim.RMSprop(self.critic_model.parameters())\n",
    "\n",
    "        \"\"\"\n",
    "        self.critic_grads = tf.gradients(self.critic_model.output, self.critic_action_input) # where we calcaulte de/        dC for feeding above\n",
    "        optimizer = optim.Adam(policy_net.parameters())\n",
    "        \"\"\"\n",
    "        \n",
    "    # ===================================================================== #\n",
    "    #                              Training                                 #\n",
    "    # ===================================================================== #    \n",
    "\n",
    "    def remember(self, cur_state, action, reward, new_state, done):\n",
    "        self.memory.append([cur_state, action, reward, new_state, done])\n",
    "    \n",
    "    def _train_actor(self, samples):\n",
    "        for real_sample in samples:\n",
    "            sample = clone_tensors(real_sample)\n",
    "            # Want to get critic grad, but with respect to the predicted action as the action input\n",
    "            cur_state, action, reward, new_state, _ = sample\n",
    "\n",
    "            # get action input\n",
    "            #self.actor_optimizer.zero_grad()\n",
    "            predicted_action = self.actor_model(cur_state)\n",
    "            \n",
    "            # get critic predictions\n",
    "            critic_input = torch.cat((cur_state, predicted_action))\n",
    "            expected_rewards = self.critic_model(critic_input)\n",
    "\n",
    "            prediction_critic_grad=torch.autograd.grad(self.critic_model.out, self.critic_model.action_input, grad_outputs=expected_rewards)[0].clone()\n",
    "\n",
    "            actor_model_weights = self.actor_model.parameters()\n",
    "            \n",
    "            # Compute actor gradients\n",
    "\n",
    "            self.actor_grads = torch.autograd.grad(self.actor_model.out, actor_model_weights, grad_outputs=-prediction_critic_grad)\n",
    "\n",
    "            # Set up relation between actor grads and weights\n",
    "            zipperino = zip(list(self.actor_model.parameters()), self.actor_grads)\n",
    "            \n",
    "            for param, gradient in zipperino:\n",
    "                param.grad = gradient.clone()\n",
    "            \n",
    "            # Step optimizer\n",
    "            #self.actor_optimizer.step()\n",
    "            \n",
    "    def _train_critic(self, samples):\n",
    "        for real_sample in samples:\n",
    "            sample = clone_tensors(real_sample)\n",
    "            cur_state, action, reward, new_state, done = sample\n",
    "            if not done:\n",
    "                target_action = self.target_actor_model(new_state)\n",
    "                # include predicted future reward in reward\n",
    "                #print(target_action)\n",
    "                critic_input = torch.cat((new_state.float(), target_action), 0)\n",
    "                future_reward = self.target_critic_model(critic_input)[0]\n",
    "                reward += self.gamma * future_reward\n",
    "            # train model\n",
    "            optim_critic_input = torch.cat((cur_state.float(), action.float()))\n",
    "            self.optimize_critic(optim_critic_input, torch.tensor(reward).view(1,1))\n",
    "    \n",
    "    def optimize_critic(self, input_state, reward):\n",
    "        # optimize critic on a batch\n",
    "\n",
    "        # get critic predictions\n",
    "        expected_rewards = self.critic_model(input_state)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.smooth_l1_loss(expected_rewards, reward.unsqueeze(1))\n",
    "        #print(loss)\n",
    "\n",
    "\n",
    "        #self.critic_model.action_input.retain_grad()\n",
    "        #loss = loss.double()\n",
    "        loss.backward(retain_graph=True)\n",
    "        #print(\"Fuck: \", self.critic_model.action_input.grad)\n",
    "        self.critic_grad = torch.autograd.grad(self.critic_model.out, self.critic_model.action_input)[0]\n",
    "\n",
    "        for param in self.critic_model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        rewards = []\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        self._train_critic(samples)\n",
    "        self._train_actor(samples)\n",
    "\n",
    "    # ========================================================================= #\n",
    "    #                         Target Model Updating                             #\n",
    "    # ========================================================================= #\n",
    "\n",
    "    def _update_actor_target(self):\n",
    "        self.target_actor_model.load_state_dict(self.actor_model.state_dict())\n",
    "\n",
    "    def _update_critic_target(self):\n",
    "        self.target_critic_model.load_state_dict(self.critic_model.state_dict())\n",
    "\n",
    "    def update_target(self):\n",
    "        self._update_actor_target()\n",
    "        self._update_critic_target()\n",
    "\n",
    "    # ========================================================================= #\n",
    "    #                              Model Predictions                            #\n",
    "    # ========================================================================= #\n",
    "\n",
    "    def act(self, cur_state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.sample_action_space()\n",
    "        \n",
    "        return self.actor_model(cur_state)\n",
    "    \n",
    "    def test(self):\n",
    "        test_states = []\n",
    "        test_actions = []\n",
    "        test_distances = []\n",
    "\n",
    "        self.env.reset()\n",
    "        state = self.env.get_state()\n",
    "        for t in count():\n",
    "            print(t)\n",
    "            #print(state)\n",
    "            # Select and perform an action\n",
    "            action = self.act(state)\n",
    "            __, reward, done, __ = self.env.step(action)\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            pred_dist, actual_dist = self.env.get_distances()\n",
    "\n",
    "            test_states.append(state)\n",
    "            test_actions.append(action[0])\n",
    "            test_distances.append([pred_dist, actual_dist])\n",
    "\n",
    "            # Observe new state\n",
    "            if not done:\n",
    "                next_state = self.env.get_state()\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done or t > 999:\n",
    "                print(t)\n",
    "                break\n",
    "            \n",
    "        measurements = np.array(test_actions) < 30\n",
    "        print(measurements)\n",
    "        #=========================================\n",
    "        # Animate Images\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ims = []\n",
    "        for i in range(len(test_states)):\n",
    "            actual_value = np.array(test_states[i].detach())[0:2]\n",
    "            distances = test_distances[i]\n",
    "            fig_title = \"a = \" + str(actual_value[0]) + \", b = \" + str(actual_value[1]) + \" test distance: \" + str(distances[0]) + \" actual distance: \" + str(distances[1])\n",
    "            x = np.array(test_states[i].detach())[2:5:2]\n",
    "            y = np.array(test_states[i].detach())[3:6:2]\n",
    "            print(\"x: \", x)\n",
    "            print(\"y: \", y)\n",
    "            if (measurements[i]):\n",
    "                print(\"measurement\")\n",
    "                \"\"\"\n",
    "                ims.append([plt.scatter(x, y, animated=True)])\n",
    "                im, = plt.plot(x, y, 'ro-', animated=True)\n",
    "                \"\"\"\n",
    "                plt.plot(x, y, 'ro-', animated=True)\n",
    "                plt.title(fig_title)\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.title(fig_title)\n",
    "                plt.scatter(x, y, animated=True)\n",
    "                plt.show()\n",
    "                #im = plt.scatter(x, y, animated=True)\n",
    "            #ims.append([im])\n",
    "\n",
    "        ani = animation.ArtistAnimation(fig, ims, interval=300, blit=True, repeat_delay=1000)\n",
    "        #plt.close()\n",
    "\n",
    "        # Show the animation\n",
    "        HTML(ani.to_jshtml())\n",
    "\n",
    "    def test_count(self):\n",
    "        self.env.reset()\n",
    "        state = self.env.get_state()\n",
    "        for t in count():\n",
    "            #print(state)\n",
    "            # Select and perform an action\n",
    "            action = self.act(state)\n",
    "            __, reward, done, __ = self.env.step(action)\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            # Observe new state\n",
    "            if not done:\n",
    "                next_state = self.env.get_state()\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done or t > 999:\n",
    "                print(\"Required \", t, \" steps\")\n",
    "                break\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(False)\n",
    "def main():\n",
    "    env = GeometryEnvironment()\n",
    "    actor_critic = ActorCritic(env)\n",
    "\n",
    "    num_trials = 2000\n",
    "    trial_len  = 999\n",
    "\n",
    "    cur_state = env.reset()\n",
    "    action = env.sample_action_space()\n",
    "    count = 0\n",
    "    while count < num_trials:\n",
    "        count += 1\n",
    "        cur_state = env.get_state()\n",
    "\n",
    "        action = actor_critic.act(cur_state)\n",
    "        #action = action[0] #action.reshape((1, env.action_space.shape[0]))\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        #new_state = new_state.reshape((1, env.observation_space.shape[0]))\n",
    "\n",
    "        actor_critic.remember(cur_state, action, reward, new_state, done)\n",
    "        actor_critic.train()\n",
    "\n",
    "        cur_state = new_state\n",
    "        if (count % 10 == 0):\n",
    "            print(\"Episode: \", count)\n",
    "            actor_critic.test_count()\n",
    "            actor_critic.update_target()\n",
    "    \n",
    "    actor_critic.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Episode:  10\nRequired  0  steps\nEpisode:  20\nRequired  205  steps\nEpisode:  30\nRequired  29  steps\nEpisode:  40\nRequired  118  steps\nEpisode:  50\nRequired  5  steps\nEpisode:  60\nRequired  52  steps\nEpisode:  70\nRequired  127  steps\nEpisode:  80\nRequired  44  steps\nEpisode:  90\nRequired  12  steps\nEpisode:  100\nRequired  8  steps\nEpisode:  110\nRequired  59  steps\nEpisode:  120\nRequired  62  steps\nEpisode:  130\nRequired  92  steps\nEpisode:  140\nRequired  1000  steps\nEpisode:  150\nRequired  90  steps\nEpisode:  160\nRequired  37  steps\nEpisode:  170\nRequired  0  steps\nEpisode:  180\nRequired  42  steps\nEpisode:  190\nRequired  20  steps\nEpisode:  200\nRequired  57  steps\nEpisode:  210\nRequired  10  steps\nEpisode:  220\nRequired  284  steps\nEpisode:  230\nRequired  131  steps\nEpisode:  240\nRequired  34  steps\nEpisode:  250\nRequired  190  steps\nEpisode:  260\nRequired  10  steps\nEpisode:  270\nRequired  25  steps\nEpisode:  280\nRequired  23  steps\nEpisode:  290\nRequired  539  steps\nEpisode:  300\nRequired  48  steps\nEpisode:  310\nRequired  116  steps\nEpisode:  320\nRequired  26  steps\nEpisode:  330\nRequired  14  steps\nEpisode:  340\nRequired  30  steps\nEpisode:  350\nRequired  6  steps\nEpisode:  360\nRequired  80  steps\nEpisode:  370\nRequired  46  steps\nEpisode:  380\nRequired  29  steps\nEpisode:  390\nRequired  134  steps\nEpisode:  400\nRequired  75  steps\nEpisode:  410\nRequired  17  steps\nEpisode:  420\nRequired  41  steps\nEpisode:  430\nRequired  129  steps\nEpisode:  440\nRequired  90  steps\nEpisode:  450\nRequired  1000  steps\nEpisode:  460\nRequired  318  steps\nEpisode:  470\nRequired  44  steps\nEpisode:  480\nRequired  4  steps\nEpisode:  490\nRequired  171  steps\nEpisode:  500\nRequired  419  steps\nEpisode:  510\nRequired  229  steps\nEpisode:  520\nRequired  31  steps\nEpisode:  530\nRequired  82  steps\nEpisode:  540\nRequired  48  steps\nEpisode:  550\nRequired  25  steps\nEpisode:  560\nRequired  23  steps\nEpisode:  570\nRequired  38  steps\nEpisode:  580\nRequired  32  steps\nEpisode:  590\nRequired  30  steps\nEpisode:  600\nRequired  13  steps\nEpisode:  610\nRequired  260  steps\nEpisode:  620\nRequired  187  steps\nEpisode:  630\nRequired  74  steps\nEpisode:  640\nRequired  38  steps\nEpisode:  650\nRequired  14  steps\nEpisode:  660\nRequired  5  steps\nEpisode:  670\nRequired  31  steps\nEpisode:  680\nRequired  173  steps\nEpisode:  690\nRequired  4  steps\nEpisode:  700\nRequired  0  steps\nEpisode:  710\nRequired  55  steps\nEpisode:  720\nRequired  5  steps\nEpisode:  730\nRequired  1000  steps\nEpisode:  740\nRequired  28  steps\nEpisode:  750\nRequired  1000  steps\nEpisode:  760\nRequired  14  steps\nEpisode:  770\nRequired  21  steps\nEpisode:  780\nRequired  43  steps\nEpisode:  790\nRequired  1000  steps\nEpisode:  800\nRequired  557  steps\nEpisode:  810\nRequired  148  steps\nEpisode:  820\nRequired  156  steps\nEpisode:  830\nRequired  158  steps\nEpisode:  840\nRequired  37  steps\nEpisode:  850\nRequired  279  steps\nEpisode:  860\nRequired  107  steps\nEpisode:  870\nRequired  2  steps\nEpisode:  880\nRequired  47  steps\nEpisode:  890\nRequired  28  steps\nEpisode:  900\nRequired  36  steps\nEpisode:  910\nRequired  54  steps\nEpisode:  920\nRequired  1000  steps\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-8b717da7a9bb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mcur_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-dfbccab9e02a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m# ========================================================================= #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-dfbccab9e02a>\u001b[0m in \u001b[0;36m_train_actor\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;31m# get action input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m#self.actor_optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mpredicted_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;31m# get critic predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/euclid/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-749fac86fbcf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m \u001b[0;31m# self.head(x.view(x.size(0), -1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/euclid/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/euclid/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/euclid/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1615\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('euclid': conda)",
   "language": "python",
   "name": "python38364biteuclidconda0c4f3e6856524f32859d293db3eeb7e9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}