{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from itertools import count\n",
    "from collections import namedtuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "\"\"\"\n",
    "Actions:\n",
    "    -Draw vertex\n",
    "    -Measure distance\n",
    "    -Measure angle\n",
    "\"\"\"\n",
    "# DEFINE ENVIRONMENT\n",
    "class GeometryEnvironment():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def place_vertex(self, bead_index, vertex_index):\n",
    "        self.bead_set[bead_index] = vertex_index\n",
    "\n",
    "    def measure_distance(self, i, j):\n",
    "        u = self.vertex_set[i]\n",
    "        v = self.vertex_set[j]\n",
    "        return euclidean(u, v)\n",
    "    \n",
    "    def reset(self, get_complex_reward = True):\n",
    "        self.get_complex_reward = get_complex_reward\n",
    "        self.bead_set = np.array([random.randint(0, 3) for i in range(2)])\n",
    "        self.defining_state = (randint(low=1, high=100), randint(low=1, high=100))\n",
    "        x = self.defining_state\n",
    "        self.vertex_set = np.array([(0,0), (x[0],0), (0,x[1]), x])\n",
    "        self.answer = 0\n",
    "        self.correct_answer = np.sqrt(self.defining_state[0]**2 + self.defining_state[1]**2)\n",
    "    \n",
    "    def basic_reward(self):\n",
    "        return -(np.abs(self.answer - self.correct_answer)**2)\n",
    "    \n",
    "    def complex_reward(self):\n",
    "        return 10.0 if self.answer == self.correct_answer else 0.0\n",
    "    \n",
    "    def step(self, action_index):\n",
    "        # if distance should be measured\n",
    "        #print(\"beads at start of step\", self.bead_set)\n",
    "        if (action_index == 8):\n",
    "            self.answer = self.measure_distance(self.bead_set[0], self.bead_set[1])\n",
    "        else:\n",
    "            vertex_index = action_index % 4\n",
    "            if (action_index < 5):\n",
    "                # move first bead\n",
    "                self.place_vertex(0, vertex_index)\n",
    "            else:\n",
    "                self.place_vertex(1, vertex_index)\n",
    "        done = self.answer == self.correct_answer\n",
    "        reward = self.complex_reward() if self.get_complex_reward else self.basic_reward()\n",
    "        return self.get_state(), reward, done, -1\n",
    "    \n",
    "    def get_state(self):\n",
    "        a = np.array(self.defining_state)\n",
    "        b = np.array(self.vertex_set[(self.bead_set[0])])\n",
    "        c = np.array(self.vertex_set[self.bead_set[1]])\n",
    "        nump = np.concatenate((a, b, c))\n",
    "        t_state = torch.tensor(nump, dtype=torch.long).float()\n",
    "        t_state.requires_grad = True\n",
    "        t_state.flatten()\n",
    "        return t_state\n",
    "    \n",
    "    def render(self):\n",
    "        #print(\"Answer: \", self.answer)\n",
    "        vertices = np.array([self.vertex_set[(self.bead_set[0])], self.vertex_set[(self.bead_set[1])]])\n",
    "        #print(vertices)\n",
    "        x, y = vertices.T\n",
    "        plt.scatter(x, y)\n",
    "        plt.show()\n",
    "    \n",
    "    def set_state(self, x, y):\n",
    "        self.defining_state = (int(x), int(y))\n",
    "        x = self.defining_state\n",
    "        self.vertex_set = np.array([(0,0), (x[0],0), (0,x[1]), x])\n",
    "        self.answer = 0\n",
    "        self.correct_answer = np.sqrt(self.defining_state[0]**2 + self.defining_state[1]**2)\n",
    "\n",
    "    \n",
    "    def sample_action_space(self):\n",
    "        random_decision = random.randint(0, 2)\n",
    "        if random_decision == 0:\n",
    "            return torch.tensor([[8]], device=device, dtype=torch.long)    \n",
    "        return torch.tensor([[random.randrange(9)]], device=device, dtype=torch.long)\n",
    "    \n",
    "    def get_distances(self):\n",
    "        # self.answer,\n",
    "        return self.measure_distance(self.bead_set[0], self.bead_set[1]), self.correct_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLAY MEMORY DEFINITION\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems:\n",
    "\n",
    "-Need to provide float args to vertex position (x and y)\n",
    "\n",
    "-Need to provide index argument to vertex position\n",
    "\n",
    "-Need to provide index arguments to distance measurement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE NETWORK\n",
    "# TODO: replace CNN with linear learner\n",
    "\n",
    "def print_with_extreme_prejudice(num):\n",
    "    print(\"Fuck you you fucking piece of shit\", num)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dimension, output_dimension):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dimension, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.head = nn.Linear(32, output_dimension)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization\n",
    "    # Returns a distribution over the possible actions tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        self.out = self.head(x.view(1, x.size(0)))[0]\n",
    "        return  self.out # self.head(x.view(x.size(0), -1))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dimension, output_dimension):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(input_dimension, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.head = nn.Linear(32, output_dimension)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.state_input = torch.randn((1, 6), requires_grad=True)\n",
    "\n",
    "        self.state1 = nn.Linear(6, 32)  # 6*6 from image dimension\n",
    "        self.state2 = nn.Linear(32, 64)\n",
    "\n",
    "        self.action_input = torch.randn((1, 1), requires_grad=True)\n",
    "        self.action_input.retain_grad()\n",
    "\n",
    "        self.merged1 = nn.Linear(65, 32)\n",
    "\n",
    "        self.head = nn.Linear(32, output_dimension)\n",
    "        self.out = torch.randn((1), requires_grad=True)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization\n",
    "    # Returns a distribution over the possible actions tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        self.action_input.register_hook(print_with_extreme_prejudice)\n",
    "        x = x.float()\n",
    "        #print(x.shape)\n",
    "        self.state_input = x[:, 0:6].clone()\n",
    "        x_state = F.relu(self.state1(self.state_input))\n",
    "        x_state = F.relu(self.state2(x_state))\n",
    "\n",
    "        self.action_input = x[:, 6].clone().unsqueeze(0)\n",
    "        x_action = self.action_input\n",
    "\n",
    "        print(x_state)\n",
    "        print(x_action)\n",
    "        x = torch.cat((x_state, x_action), 1)\n",
    "        x = F.relu(self.merged1(x))\n",
    "        self.out = self.head(x.view(x.size(0), -1)) # self.head(x.view(x.size(0), -1))\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "def clone_tensors(item_list):\n",
    "    new_items = []\n",
    "    for item in item_list:\n",
    "        new_items.append(item.clone() if torch.is_tensor(item) else item)\n",
    "    return new_items\n",
    "\n",
    "class ActorCritic:\n",
    "    def __init__(self, env):\n",
    "        self.env  = env\n",
    "\n",
    "        self.action_dimension = 9\n",
    "        self.environment_dimension = 6     \n",
    "        \n",
    "        self.learning_rate = 0.01 #0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 50 #.995\n",
    "        self.gamma = 0.9 #.95\n",
    "        self.tau   = .125\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # ===================================================================== #\n",
    "        #                               Actor Model                             #\n",
    "        # Chain rule: find the gradient of changing the actor network params in #\n",
    "        # getting closest to the final value network predictions, i.e. de/dA    #\n",
    "        # Calculate de/dA as = de/dC * dC/dA, where e is error, C critic, A act #\n",
    "        # ===================================================================== #\n",
    "\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        self.actor_model = Actor(self.environment_dimension, self.action_dimension).float()\n",
    "        self.target_actor_model = Actor(self.environment_dimension, self.action_dimension).float()\n",
    "\n",
    "        self.target_actor_model.load_state_dict(self.actor_model.state_dict())\n",
    "        self.target_actor_model.eval()\n",
    "\n",
    "        #self.actor_optimizer = optim.Adam(self.actor_model.parameters())\n",
    "        self.actor_optimizer = optim.RMSprop(self.actor_model.parameters())\n",
    "        \n",
    "        #print(self.actor_model.state_dict())\n",
    "        \"\"\"\n",
    "        self.actor_critic_grad = tf.placeholder(tf.float32, \n",
    "            [None, self.env.action_space.shape[0]]) # where we will feed de/dC (from critic)\n",
    "        \n",
    "        actor_model_weights = self.actor_model.trainable_weights\n",
    "        self.actor_grads = tf.gradients(self.actor_model.output, \n",
    "            actor_model_weights, -self.actor_critic_grad) # dC/dA (from actor)\n",
    "        \n",
    "        grads = zip(self.actor_grads, actor_model_weights)\n",
    "\n",
    "        \n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)\n",
    "         optimizer = optim.Adam(policy_net.parameters())\n",
    "         \"\"\"\n",
    "\n",
    "        # ===================================================================== #\n",
    "        #                              Critic Model                             #\n",
    "        # ===================================================================== #        \n",
    "\n",
    "        self.critic_model = Critic(self.action_dimension + self.environment_dimension, 1).float()\n",
    "        self.target_critic_model = Critic(self.action_dimension + self.environment_dimension, 1).float()\n",
    "\n",
    "        self.target_critic_model.load_state_dict(self.critic_model.state_dict())\n",
    "        self.target_critic_model.eval()\n",
    "\n",
    "        #self.critic_grads =  torch.autograd.grad(self.critic_model.head, self.critic_model.action_input)\n",
    "\n",
    "        #self.critic_optimizer = optim.Adam(self.critic_model.parameters())\n",
    "        self.critic_optimizer = optim.RMSprop(self.critic_model.parameters())\n",
    "\n",
    "        \"\"\"\n",
    "        self.critic_grads = tf.gradients(self.critic_model.output, self.critic_action_input) # where we calcaulte de/        dC for feeding above\n",
    "        optimizer = optim.Adam(policy_net.parameters())\n",
    "        \"\"\"\n",
    "        \n",
    "    # ===================================================================== #\n",
    "    #                              Training                                 #\n",
    "    # ===================================================================== #    \n",
    "\n",
    "    def remember(self, cur_state, action, reward, new_state, done):\n",
    "        self.memory.append([cur_state, action, reward, new_state, done])\n",
    "    \n",
    "    def _train_actor(self, samples):\n",
    "        for real_sample in samples:\n",
    "            sample = clone_tensors(real_sample)\n",
    "            # Want to get critic grad, but with respect to the predicted action as the action input\n",
    "            cur_state, action, reward, new_state, _ = sample\n",
    "\n",
    "            # get action input\n",
    "            #self.actor_optimizer.zero_grad()\n",
    "            predicted_action = self.actor_model(cur_state)\n",
    "            predicted_action = predicted_action.max(0)[1].view(1, 1).float()\n",
    "            \n",
    "            # get critic predictions\n",
    "            critic_input = torch.cat((cur_state, predicted_action.flatten()))\n",
    "            expected_rewards = self.critic_model(critic_input.unsqueeze(0))\n",
    "\n",
    "            prediction_critic_grad = torch.autograd.grad(self.critic_model.out, self.critic_model.action_input, grad_outputs=expected_rewards)[0].clone()\n",
    "\n",
    "            actor_model_weights = self.actor_model.parameters()\n",
    "            \n",
    "            # Compute actor gradients\n",
    "\n",
    "            self.actor_grads = torch.autograd.grad(self.actor_model.out, actor_model_weights, grad_outputs=-prediction_critic_grad)\n",
    "\n",
    "            # Set up relation between actor grads and weights\n",
    "            zipperino = zip(list(self.actor_model.parameters()), self.actor_grads)\n",
    "            \n",
    "            for param, gradient in zipperino:\n",
    "                param.grad = gradient.clone()\n",
    "            \n",
    "            # Step optimizer\n",
    "            #self.actor_optimizer.step()\n",
    "            \n",
    "    def _train_critic(self, samples):\n",
    "        for real_sample in samples:\n",
    "            sample = clone_tensors(real_sample)\n",
    "            cur_state, action, reward, new_state, done = sample\n",
    "            if not done:\n",
    "                target_action = self.target_actor_model(new_state)\n",
    "                # get actual value of target action\n",
    "                target_action = target_action.max(0)[1].view(1, 1).float()\n",
    "                # include predicted future reward in reward\n",
    "                #print(target_action)\n",
    "                critic_input = torch.cat((new_state.float(), target_action.flatten()), 0)\n",
    "                print(critic_input)\n",
    "                future_reward = self.target_critic_model(critic_input.unsqueeze(0))[0]\n",
    "                reward += self.gamma * future_reward\n",
    "            # train model\n",
    "            optim_critic_input = torch.cat((cur_state.float(), action.flatten().float()), axis=0)\n",
    "\n",
    "            self.optimize_critic(optim_critic_input, torch.tensor(reward).view(1,1))\n",
    "    \n",
    "    def optimize_critic(self, input_state, reward):\n",
    "        # optimize critic on a batch\n",
    "        print(input_state)\n",
    "\n",
    "        # get critic predictions\n",
    "        expected_rewards = self.critic_model(input_state.unsqueeze(0))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.smooth_l1_loss(expected_rewards, reward.unsqueeze(1))\n",
    "        #print(loss)\n",
    "\n",
    "\n",
    "        #self.critic_model.action_input.retain_grad()\n",
    "        #loss = loss.double()\n",
    "        loss.backward(retain_graph=True)\n",
    "        #print(\"Fuck: \", self.critic_model.action_input.grad)\n",
    "        self.critic_grad = torch.autograd.grad(self.critic_model.out, self.critic_model.action_input)[0]\n",
    "\n",
    "        for param in self.critic_model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        rewards = []\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        self._train_critic(samples)\n",
    "        self._train_actor(samples)\n",
    "\n",
    "    # ========================================================================= #\n",
    "    #                         Target Model Updating                             #\n",
    "    # ========================================================================= #\n",
    "\n",
    "    def _update_actor_target(self):\n",
    "        self.target_actor_model.load_state_dict(self.actor_model.state_dict())\n",
    "\n",
    "    def _update_critic_target(self):\n",
    "        self.target_critic_model.load_state_dict(self.critic_model.state_dict())\n",
    "\n",
    "    def update_target(self):\n",
    "        self._update_actor_target()\n",
    "        self._update_critic_target()\n",
    "\n",
    "    # ========================================================================= #\n",
    "    #                              Model Predictions                            #\n",
    "    # ========================================================================= #\n",
    "\n",
    "    def act(self, cur_state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.sample_action_space()\n",
    "        \n",
    "        return self.actor_model(cur_state)\n",
    "    \n",
    "    def test(self):\n",
    "        test_states = []\n",
    "        test_actions = []\n",
    "        test_distances = []\n",
    "\n",
    "        self.env.reset()\n",
    "        state = self.env.get_state()\n",
    "        for t in count():\n",
    "            print(t)\n",
    "            #print(state)\n",
    "            # Select and perform an action\n",
    "            action = self.act(state)\n",
    "            __, reward, done, __ = self.env.step(action)\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            pred_dist, actual_dist = self.env.get_distances()\n",
    "\n",
    "            test_states.append(state)\n",
    "            test_actions.append(action[0])\n",
    "            test_distances.append([pred_dist, actual_dist])\n",
    "\n",
    "            # Observe new state\n",
    "            if not done:\n",
    "                next_state = self.env.get_state()\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done or t > 999:\n",
    "                print(t)\n",
    "                break\n",
    "            \n",
    "        measurements = np.array(test_actions) < 30\n",
    "        print(measurements)\n",
    "        #=========================================\n",
    "        # Animate Images\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ims = []\n",
    "        for i in range(len(test_states)):\n",
    "            actual_value = np.array(test_states[i].detach())[0:2]\n",
    "            distances = test_distances[i]\n",
    "            fig_title = \"a = \" + str(actual_value[0]) + \", b = \" + str(actual_value[1]) + \" test distance: \" + str(distances[0]) + \" actual distance: \" + str(distances[1])\n",
    "            x = np.array(test_states[i].detach())[2:5:2]\n",
    "            y = np.array(test_states[i].detach())[3:6:2]\n",
    "            print(\"x: \", x)\n",
    "            print(\"y: \", y)\n",
    "            if (measurements[i]):\n",
    "                print(\"measurement\")\n",
    "                \"\"\"\n",
    "                ims.append([plt.scatter(x, y, animated=True)])\n",
    "                im, = plt.plot(x, y, 'ro-', animated=True)\n",
    "                \"\"\"\n",
    "                plt.plot(x, y, 'ro-', animated=True)\n",
    "                plt.title(fig_title)\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.title(fig_title)\n",
    "                plt.scatter(x, y, animated=True)\n",
    "                plt.show()\n",
    "                #im = plt.scatter(x, y, animated=True)\n",
    "            #ims.append([im])\n",
    "\n",
    "        ani = animation.ArtistAnimation(fig, ims, interval=300, blit=True, repeat_delay=1000)\n",
    "        #plt.close()\n",
    "\n",
    "        # Show the animation\n",
    "        HTML(ani.to_jshtml())\n",
    "\n",
    "    def test_count(self):\n",
    "        self.env.reset()\n",
    "        state = self.env.get_state()\n",
    "        for t in count():\n",
    "            #print(state)\n",
    "            # Select and perform an action\n",
    "            action = self.act(state)\n",
    "            __, reward, done, __ = self.env.step(action)\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            # Observe new state\n",
    "            if not done:\n",
    "                next_state = self.env.get_state()\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done or t > 999:\n",
    "                print(\"Required \", t, \" steps\")\n",
    "                break\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(False)\n",
    "def main():\n",
    "    env = GeometryEnvironment()\n",
    "    actor_critic = ActorCritic(env)\n",
    "\n",
    "    num_trials = 2000\n",
    "    trial_len  = 999\n",
    "\n",
    "    cur_state = env.reset()\n",
    "    action = env.sample_action_space()\n",
    "    count = 0\n",
    "    while count < num_trials:\n",
    "        count += 1\n",
    "        cur_state = env.get_state()\n",
    "\n",
    "        action = actor_critic.act(cur_state)\n",
    "        #action = action[0] #action.reshape((1, env.action_space.shape[0]))\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        #new_state = new_state.reshape((1, env.observation_space.shape[0]))\n",
    "\n",
    "        actor_critic.remember(cur_state, action, reward, new_state, done)\n",
    "        actor_critic.train()\n",
    "\n",
    "        cur_state = new_state\n",
    "        if (count % 10 == 0):\n",
    "            print(\"Episode: \", count)\n",
    "            actor_critic.test_count()\n",
    "            actor_critic.update_target()\n",
    "    \n",
    "    actor_critic.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Episode:  10\nRequired  20  steps\nEpisode:  20\nRequired  2  steps\nEpisode:  30\nRequired  7  steps\ntensor([23., 64.,  0.,  0., 23.,  0.,  1.], grad_fn=<CatBackward>)\ntensor([[15.0448,  1.1981,  4.5461,  1.9848,  6.2570,  0.0000,  0.0000, 10.7930,\n          0.0000,  0.0000,  7.3734,  0.0000,  0.0000,  0.0000, 20.9615,  0.0000,\n         10.8520,  0.0000,  0.0000,  0.0000,  4.7871, 10.0066, 11.5203,  0.6246,\n          0.0866,  0.6717,  0.0000,  9.2722,  0.0000,  0.0000,  2.4353,  5.4947,\n          0.0000,  8.9224, 12.9772,  0.0000,  0.0382,  0.4007,  5.0107,  0.0000,\n          0.0000,  0.4666,  0.7026,  0.0000,  0.3458,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  4.1490,  0.0000,  0.0000, 12.8572,  0.0000,\n          0.0000,  0.0000,  6.1417, 11.2532,  0.0000, 13.7491,  7.1651,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64.,  0.,  0., 23.,  0.,  8.], grad_fn=<CatBackward>)\ntensor([[15.0448,  1.1981,  4.5461,  1.9848,  6.2570,  0.0000,  0.0000, 10.7930,\n          0.0000,  0.0000,  7.3734,  0.0000,  0.0000,  0.0000, 20.9615,  0.0000,\n         10.8520,  0.0000,  0.0000,  0.0000,  4.7871, 10.0066, 11.5203,  0.6246,\n          0.0866,  0.6717,  0.0000,  9.2722,  0.0000,  0.0000,  2.4353,  5.4947,\n          0.0000,  8.9224, 12.9772,  0.0000,  0.0382,  0.4007,  5.0107,  0.0000,\n          0.0000,  0.4666,  0.7026,  0.0000,  0.3458,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  4.1490,  0.0000,  0.0000, 12.8572,  0.0000,\n          0.0000,  0.0000,  6.1417, 11.2532,  0.0000, 13.7491,  7.1651,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[8.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64.,  0.,  0., 23.,  0.,  1.], grad_fn=<CatBackward>)\ntensor([[15.0448,  1.1981,  4.5461,  1.9848,  6.2570,  0.0000,  0.0000, 10.7930,\n          0.0000,  0.0000,  7.3734,  0.0000,  0.0000,  0.0000, 20.9615,  0.0000,\n         10.8520,  0.0000,  0.0000,  0.0000,  4.7871, 10.0066, 11.5203,  0.6246,\n          0.0866,  0.6717,  0.0000,  9.2722,  0.0000,  0.0000,  2.4353,  5.4947,\n          0.0000,  8.9224, 12.9772,  0.0000,  0.0382,  0.4007,  5.0107,  0.0000,\n          0.0000,  0.4666,  0.7026,  0.0000,  0.3458,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  4.1490,  0.0000,  0.0000, 12.8572,  0.0000,\n          0.0000,  0.0000,  6.1417, 11.2532,  0.0000, 13.7491,  7.1651,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64.,  0.,  0., 23.,  0.,  8.], grad_fn=<CatBackward>)\ntensor([[48.8469, 31.6929,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 52.7786,  0.0000,\n         38.2506,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 43.4224,  0.0000,\n         27.9066,  0.0000,  0.0000, 29.7547,  0.0000,  0.0000,  0.0000, 40.8982,\n          0.0000, 40.0593,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2903,\n          0.0000, 28.2009, 37.6430,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          4.0242,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 45.1660,  2.1165]],\n       grad_fn=<ReluBackward0>)\ntensor([[8.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64., 23., 64.,  0., 64.,  1.], grad_fn=<CatBackward>)\ntensor([[23.9695,  0.0000, 12.7104,  1.0070, 11.8735,  0.0000,  0.0000,  9.3579,\n          0.0000,  3.4763,  0.0000,  4.1676,  0.0000,  0.0000, 17.0596,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 14.0236, 10.2989,  0.0000,\n          0.0000,  0.0000,  1.6917,  9.8678,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  9.6992, 19.1630,  0.0000,  0.3461,  0.0000,  5.2313,  0.0000,\n          0.0000,  2.0361,  7.6731,  6.3751,  3.5434,  0.3273,  0.0000,  0.0000,\n         10.0954,  0.0000,  4.9330, 11.5333,  0.0000,  0.2256, 10.0394,  0.0000,\n          0.6339,  0.0000,  0.0000, 11.6646,  0.0000, 11.1913,  0.0000,  6.7766]],\n       grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64.,  0.,  0.,  0., 64.,  3.], grad_fn=<CatBackward>)\ntensor([[21.8360,  4.1966,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  4.7262,  0.0000,  2.0295,  0.0000,  0.0000, 17.6718,  0.0000,\n          5.2646,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 12.2624,  0.0000,\n          0.0000,  0.0000,  1.9285,  4.8580,  0.0000,  0.0000,  0.0000,  1.5224,\n          0.0000,  5.2781,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 18.2172,\n          0.0000,  0.0000,  8.3144,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.8360,  1.1337,  0.0000,  0.0000,  4.7401,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  7.7997,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[3.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64.,  0.,  0., 23.,  0.,  1.], grad_fn=<CatBackward>)\ntensor([[15.0448,  1.1981,  4.5461,  1.9848,  6.2570,  0.0000,  0.0000, 10.7930,\n          0.0000,  0.0000,  7.3734,  0.0000,  0.0000,  0.0000, 20.9615,  0.0000,\n         10.8520,  0.0000,  0.0000,  0.0000,  4.7871, 10.0066, 11.5203,  0.6246,\n          0.0866,  0.6717,  0.0000,  9.2722,  0.0000,  0.0000,  2.4353,  5.4947,\n          0.0000,  8.9224, 12.9772,  0.0000,  0.0382,  0.4007,  5.0107,  0.0000,\n          0.0000,  0.4666,  0.7026,  0.0000,  0.3458,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  4.1490,  0.0000,  0.0000, 12.8572,  0.0000,\n          0.0000,  0.0000,  6.1417, 11.2532,  0.0000, 13.7491,  7.1651,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64.,  0.,  0., 23.,  0.,  8.], grad_fn=<CatBackward>)\ntensor([[ 7.2775,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  6.7823,  0.0000, 10.1482,  0.0000,  0.4450, 12.8985,  0.0000,\n          2.7470,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 11.8430,  0.0000,\n          0.0000,  0.0000,  0.0000,  3.4162,  0.0000,  0.0000,  0.0000,  0.9079,\n          0.0000,  9.7908,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1903,\n          0.0000,  0.3799,  5.5957,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  9.7483,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  9.1366,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[8.]], grad_fn=<UnsqueezeBackward0>)\ntensor([66., 24., 66.,  0.,  0., 24.,  5.], grad_fn=<CatBackward>)\ntensor([[ 9.0530,  8.2165,  5.9723,  0.0000, 16.0064,  0.0000,  0.0000,  7.5477,\n          5.8221,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 20.2083,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  5.1951,  4.5314,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  3.8889,\n          0.0000,  3.3872,  5.2389,  0.0000,  0.9846,  7.8418,  0.0000,  5.9627,\n          4.3686,  0.0000,  1.3653,  1.2527,  0.0000,  0.0000,  0.0000,  0.0000,\n          7.3739,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  8.0642,  0.0000,\n          0.0000,  0.3730,  6.6865, 11.9806,  2.1990, 12.0731,  0.0000,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[5.]], grad_fn=<UnsqueezeBackward0>)\ntensor([66., 24., 66.,  0., 66., 24.,  6.], grad_fn=<CatBackward>)\ntensor([[0.0000, 4.3657, 0.0000, 0.0000, 1.2855, 4.7229, 0.0000, 0.0000, 4.1975,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 8.6848, 1.0580, 1.9902, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 4.2367, 4.1607, 3.6055, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.7341, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 2.4468, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 4.0092, 0.0000, 1.1600, 3.7600, 0.0000, 0.0000, 0.0000,\n         0.0000]], grad_fn=<ReluBackward0>)\ntensor([[6.]], grad_fn=<UnsqueezeBackward0>)\ntensor([66., 24., 66.,  0.,  0., 24.,  8.], grad_fn=<CatBackward>)\ntensor([[ 1.0796,  0.0000,  0.0000,  0.0000,  0.0000,  5.6510,  0.0000,  0.0000,\n         10.3738,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9335,  0.0000,\n          0.0000,  0.0000,  1.8116,  0.3090,  0.0000,  0.0000,  0.0000,  0.0000,\n          1.2427,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.2533,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.9014,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  9.6808,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.2648,  0.0000,  0.0000,  5.2688,  0.0000,  0.0000,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[8.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64.,  0.,  0.,  0., 64.,  1.], grad_fn=<CatBackward>)\ntensor([[24.8739,  2.2177,  4.6950,  4.1270,  7.3489,  0.0000,  0.0000,  3.3816,\n          0.0000,  5.3558,  4.6020,  1.3244,  0.0000,  0.0000, 17.3001,  0.0000,\n          4.3186,  0.0000,  0.0000,  0.0000,  0.0000, 11.2052, 10.9692,  0.0000,\n          0.0000,  0.0000,  0.6102,  7.7246,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  3.0928, 20.2351,  0.0000,  0.0000,  0.0000,  0.0000,  3.2151,\n          0.0000,  0.0000,  1.9738,  0.0000,  0.7396,  0.0000,  0.0000,  0.0000,\n          3.6805,  0.7747,  0.0000,  7.8705,  0.0000,  9.4328, 11.3209,  0.0000,\n          0.0000,  0.0000,  3.0085,  9.2320,  0.0000, 12.2615,  2.3453,  7.3227]],\n       grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64.,  0.,  0., 23.,  0.,  6.], grad_fn=<CatBackward>)\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 4.2500, 0.0000, 0.0000, 0.2373, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3714, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4291,\n         0.0000]], grad_fn=<ReluBackward0>)\ntensor([[6.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64.,  0.,  0., 23., 64.,  5.], grad_fn=<CatBackward>)\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         4.8852, 0.0000, 9.1402, 1.8762, 0.0000, 1.7915, 0.0000, 0.0000, 0.0000,\n         0.0000, 8.5521, 0.0000, 0.0000, 8.7673, 0.0000, 0.0000, 0.0000, 0.0000,\n         5.7822, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 8.2734, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.2978, 0.0000,\n         1.0464, 0.0000, 0.3275, 0.0000, 9.3624, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 1.8383, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 6.5088,\n         0.1877]], grad_fn=<ReluBackward0>)\ntensor([[5.]], grad_fn=<UnsqueezeBackward0>)\ntensor([19., 55.,  0., 55.,  0., 55.,  1.], grad_fn=<CatBackward>)\ntensor([[21.8494,  0.0000, 11.8554,  0.7681, 10.9343,  0.0000,  0.0000,  9.3148,\n          0.0000,  3.2178,  2.0691,  4.1884,  0.0000,  0.0000, 13.9170,  0.0000,\n          2.3276,  0.0000,  0.0000,  0.0000,  0.0000, 12.6455,  9.0650,  0.0000,\n          0.0000,  0.0000,  0.6200, 11.3003,  0.0000,  0.0000,  0.0411,  0.0000,\n          0.0000,  8.8055, 17.8411,  0.0000,  0.0000,  0.0000,  5.2548,  0.0000,\n          0.0000,  0.6008,  4.1258,  4.9320,  4.3176,  0.5350,  0.0000,  0.0000,\n          5.5915,  0.0000,  2.3607, 10.5476,  0.0000,  1.3212,  9.5543,  0.0000,\n          0.0000,  0.0000,  0.0000, 11.7920,  0.0000, 12.0662,  4.0743,  5.6492]],\n       grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([19., 55.,  0., 55.,  0., 55.,  8.], grad_fn=<CatBackward>)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000, 19.6197,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000, 24.9775,  0.0000,  0.0000, 14.2050,  0.0000,\n          0.0000,  0.0000,  0.0000, 10.6957,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000, 16.1692,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 13.3746,  0.8759,  0.0000,\n          0.0000, 14.9947,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 16.6427,\n          4.2883,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  9.3015,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[8.]], grad_fn=<UnsqueezeBackward0>)\ntensor([19., 55., 19.,  0.,  0., 55.,  1.], grad_fn=<CatBackward>)\ntensor([[2.4602e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.7936e+00,\n         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 7.7488e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3411e+00, 2.6805e-01, 0.0000e+00,\n         1.6074e+00, 0.0000e+00, 0.0000e+00, 8.3537e-01, 0.0000e+00, 6.7964e-03,\n         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.2009e+00, 0.0000e+00,\n         1.4161e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 2.3754e+00, 6.7622e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00, 2.0909e+00, 0.0000e+00]],\n       grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([66., 24., 66.,  0., 66., 24.,  1.], grad_fn=<CatBackward>)\ntensor([[ 6.8494, 20.1382, 12.3338,  4.4590, 15.7123,  0.0000,  0.0000, 20.0003,\n         13.1407,  0.0000,  7.4421,  0.0000,  0.0000,  0.0000, 26.0737,  0.0000,\n         10.1221,  0.0000,  0.0000,  0.0000,  0.0000,  8.8454,  7.6171, 11.7550,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  7.6984,\n          0.0000, 12.3038,  7.3928,  0.0000,  0.0000, 10.2044,  0.0000,  0.0000,\n          1.1802,  0.0000,  0.0000,  0.0000,  2.8368,  0.0000,  0.0000,  0.0000,\n         13.6422,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  8.3189,  0.0000,\n          0.0000,  0.0000, 13.4783, 21.1170,  0.0000, 12.8512,  0.0000,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([66., 24., 66.,  0., 66., 24.,  8.], grad_fn=<CatBackward>)\ntensor([[ 5.4946,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          6.4691,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000, 11.4411,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  1.6157,  6.5067,  0.0000,  0.0000,  0.0000,  0.0000,  3.4815,\n          0.0000,  7.1430,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          7.4790,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[8.]], grad_fn=<UnsqueezeBackward0>)\ntensor([19., 55.,  0.,  0., 19., 55.,  6.], grad_fn=<CatBackward>)\ntensor([[11.5176,  0.0000,  0.0000,  0.2634,  0.0000,  0.3723,  0.0000,  0.0000,\n          4.5396,  0.0000,  0.0000,  0.0729,  0.0000,  0.0000,  0.0000,  3.7383,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  2.4088,  0.0000,  0.0000,  0.0000,  0.0000,  2.0510,  0.0000,\n         14.6057,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[6.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64.,  0.,  0.,  0., 64.,  1.], grad_fn=<CatBackward>)\ntensor([[24.8739,  2.2177,  4.6950,  4.1270,  7.3489,  0.0000,  0.0000,  3.3816,\n          0.0000,  5.3558,  4.6020,  1.3244,  0.0000,  0.0000, 17.3001,  0.0000,\n          4.3186,  0.0000,  0.0000,  0.0000,  0.0000, 11.2052, 10.9692,  0.0000,\n          0.0000,  0.0000,  0.6102,  7.7246,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  3.0928, 20.2351,  0.0000,  0.0000,  0.0000,  0.0000,  3.2151,\n          0.0000,  0.0000,  1.9738,  0.0000,  0.7396,  0.0000,  0.0000,  0.0000,\n          3.6805,  0.7747,  0.0000,  7.8705,  0.0000,  9.4328, 11.3209,  0.0000,\n          0.0000,  0.0000,  3.0085,  9.2320,  0.0000, 12.2615,  2.3453,  7.3227]],\n       grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64.,  0.,  0.,  0., 64.,  0.], grad_fn=<CatBackward>)\ntensor([[5.4667, 0.0000, 0.0000, 2.5859, 0.0000, 1.9741, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 4.0113, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.3621, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7712, 0.0000, 0.0000, 0.0000,\n         0.0000, 3.5683, 0.0000, 7.1478, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000]], grad_fn=<ReluBackward0>)\ntensor([[0.]], grad_fn=<UnsqueezeBackward0>)\ntensor([66., 24., 66.,  0.,  0., 24.,  8.], grad_fn=<CatBackward>)\ntensor([[11.2453,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          3.8004,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.5375,\n          6.2711,  4.2957,  0.0000,  0.0000,  0.0000,  0.0000,  3.4937,  0.0000,\n         16.8426,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0390,  0.0000,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[8.]], grad_fn=<UnsqueezeBackward0>)\ntensor([66., 24., 66.,  0., 66., 24.,  1.], grad_fn=<CatBackward>)\ntensor([[ 6.8494, 20.1382, 12.3338,  4.4590, 15.7123,  0.0000,  0.0000, 20.0003,\n         13.1407,  0.0000,  7.4421,  0.0000,  0.0000,  0.0000, 26.0737,  0.0000,\n         10.1221,  0.0000,  0.0000,  0.0000,  0.0000,  8.8454,  7.6171, 11.7550,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  7.6984,\n          0.0000, 12.3038,  7.3928,  0.0000,  0.0000, 10.2044,  0.0000,  0.0000,\n          1.1802,  0.0000,  0.0000,  0.0000,  2.8368,  0.0000,  0.0000,  0.0000,\n         13.6422,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  8.3189,  0.0000,\n          0.0000,  0.0000, 13.4783, 21.1170,  0.0000, 12.8512,  0.0000,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([66., 24.,  0., 24., 66., 24.,  1.], grad_fn=<CatBackward>)\ntensor([[4.5061, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 8.5006,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.4164, 0.0000, 0.0090, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 2.8652, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 8.3044, 0.0000, 7.8194, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.7338, 0.0000,\n         0.0000]], grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([66., 24., 66.,  0.,  0., 24.,  3.], grad_fn=<CatBackward>)\ntensor([[ 3.4426,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          9.1254,  2.7233,  0.0000,  0.0000,  0.0000,  0.0000,  0.3591,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  6.9058,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 11.8161,\n         22.8161,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.7703,  0.0000,\n          0.9187,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.3551,  0.0000,  0.0000,  0.0000,  0.0000, 14.0293,  0.0000,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[3.]], grad_fn=<UnsqueezeBackward0>)\ntensor([19., 55.,  0.,  0.,  0., 55.,  1.], grad_fn=<CatBackward>)\ntensor([[2.1851, 0.0000, 0.0000, 0.7567, 0.0000, 1.6674, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 5.1607, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 1.2030, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.1383, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 1.4704, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.8251, 0.0000, 3.7361, 2.0696, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000]], grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([66., 24., 66.,  0.,  0., 24.,  8.], grad_fn=<CatBackward>)\ntensor([[ 3.6952,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          5.3960, 11.3865,  0.0000,  0.0000,  0.0000,  0.0000,  2.6982,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.0773,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  8.2992,\n         12.2929,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.1364,  0.0000,\n          0.0000,  0.0000,  0.3613,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          8.8212,  0.0000,  0.0000,  0.0000,  0.0000, 20.3366,  0.0000,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[8.]], grad_fn=<UnsqueezeBackward0>)\ntensor([19., 55., 19.,  0.,  0., 55.,  1.], grad_fn=<CatBackward>)\ntensor([[ 7.1724,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  1.9967,  0.0000,  1.0476,  0.0000,  0.0000,  0.8167,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.6968,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1017,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         11.1073,  0.0000,  1.4424,  0.0000,  0.0000,  0.0000,  2.4852,  0.0000,\n          1.4336,  0.0000,  4.3719,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          1.0378,  0.0000,  0.0000,  0.0000,  0.0000, 12.7139,  0.0000,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64.,  0.,  0., 23.,  0.,  1.], grad_fn=<CatBackward>)\ntensor([[15.0448,  1.1981,  4.5461,  1.9848,  6.2570,  0.0000,  0.0000, 10.7930,\n          0.0000,  0.0000,  7.3734,  0.0000,  0.0000,  0.0000, 20.9615,  0.0000,\n         10.8520,  0.0000,  0.0000,  0.0000,  4.7871, 10.0066, 11.5203,  0.6246,\n          0.0866,  0.6717,  0.0000,  9.2722,  0.0000,  0.0000,  2.4353,  5.4947,\n          0.0000,  8.9224, 12.9772,  0.0000,  0.0382,  0.4007,  5.0107,  0.0000,\n          0.0000,  0.4666,  0.7026,  0.0000,  0.3458,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  4.1490,  0.0000,  0.0000, 12.8572,  0.0000,\n          0.0000,  0.0000,  6.1417, 11.2532,  0.0000, 13.7491,  7.1651,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64.,  0.,  0., 23.,  0.,  8.], grad_fn=<CatBackward>)\ntensor([[ 4.7022,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.2177,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.9448,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  8.4350,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         22.2071,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  5.4150,  0.0000,\n          0.0000,  0.0000,  4.6606,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 13.7036,  0.1226,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[8.]], grad_fn=<UnsqueezeBackward0>)\ntensor([19., 55., 19.,  0.,  0., 55.,  2.], grad_fn=<CatBackward>)\ntensor([[10.2448,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.9855,  0.0000,\n          0.0000, 11.7390,  0.0000,  4.0647,  0.0000,  0.0000,  1.1846,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  3.4377,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.9963,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         22.3455,  0.0000,  3.7930,  0.0000,  0.0000,  0.0000,  3.3026,  0.0000,\n          0.0000,  0.0000,  6.9932,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 20.6448,  0.0000,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[2.]], grad_fn=<UnsqueezeBackward0>)\ntensor([19., 55.,  0.,  0.,  0., 55.,  1.], grad_fn=<CatBackward>)\ntensor([[21.2956,  1.9338,  4.0114,  3.5875,  6.3310,  0.0000,  0.0000,  2.7099,\n          0.0000,  4.7149,  3.8665,  1.2124,  0.0000,  0.0000, 14.8558,  0.0000,\n          3.6330,  0.0000,  0.0000,  0.0000,  0.0000,  9.6151,  9.3844,  0.0000,\n          0.0000,  0.0000,  0.5762,  6.5571,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  2.6465, 17.2808,  0.0000,  0.0000,  0.0000,  0.0000,  2.8762,\n          0.0000,  0.0000,  1.7288,  0.0000,  0.6454,  0.0000,  0.0000,  0.0000,\n          3.1737,  0.6038,  0.0000,  6.8152,  0.0000,  8.1793,  9.5799,  0.0000,\n          0.0000,  0.0000,  2.5591,  7.8555,  0.0000, 10.4027,  1.9658,  6.3802]],\n       grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([19., 55.,  0., 55.,  0., 55.,  4.], grad_fn=<CatBackward>)\ntensor([[ 8.3131,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.6203,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.8890,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         12.6096,  0.9187,  0.0000,  0.0000,  0.0000,  0.0000,  6.6776,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          6.3655,  0.0000,  0.0000,  0.0000,  0.0000,  3.4055,  0.0000,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[4.]], grad_fn=<UnsqueezeBackward0>)\ntensor([19., 55., 19.,  0.,  0., 55.,  8.], grad_fn=<CatBackward>)\ntensor([[0.2562, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.6707, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5418, 0.0000,\n         0.0000, 0.0000, 0.0000, 1.0958, 2.5901, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 1.6365, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.9556, 0.0000,\n         0.0000]], grad_fn=<ReluBackward0>)\ntensor([[8.]], grad_fn=<UnsqueezeBackward0>)\ntensor([66., 24., 66.,  0.,  0., 24.,  8.], grad_fn=<CatBackward>)\ntensor([[ 3.3209,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  7.6511,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  8.5646,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.1967,\n         15.4895,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  1.1400,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          1.9015,  0.0000,  0.0000,  0.0000,  0.0000, 22.9870,  0.0000,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[8.]], grad_fn=<UnsqueezeBackward0>)\ntensor([19., 55.,  0.,  0., 19., 55.,  1.], grad_fn=<CatBackward>)\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 6.5706,\n         0.0000, 0.0000, 4.4739, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 5.0267, 0.0000, 0.0000, 4.8661, 0.0000, 0.0000, 0.0000, 0.0000,\n         1.5288, 0.0000, 0.0000, 0.0000, 0.6491, 0.0000, 1.2203, 2.1811, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 6.8243, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0288, 2.7521, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.5864,\n         0.0000]], grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([74., 18.,  0., 18., 74.,  0.,  8.], grad_fn=<CatBackward>)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.1952,  0.0000,  0.0000,  7.4246,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.3594,\n          0.0000,  2.1031,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         16.9537,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          4.1818,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.5611,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[8.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64., 23., 64., 23.,  0.,  1.], grad_fn=<CatBackward>)\ntensor([[21.4557,  0.0000, 11.4315,  4.1738,  3.8006,  0.0000,  0.0000, 17.8035,\n          0.0000,  0.0000,  3.1704,  0.0000,  0.0000,  0.0000, 14.6186,  0.0000,\n          4.2487,  0.0000,  0.0000,  1.3913,  0.0000, 13.6283, 13.1545,  0.0000,\n          0.0000,  1.9656,  0.0000, 16.8714,  0.0000,  0.0000, 12.2707, 11.3109,\n          0.0000, 15.0076, 17.0175,  0.0000,  3.3894,  0.0000,  6.1144,  0.0000,\n          0.0000, 10.0340,  3.7527,  0.0000,  0.0000,  0.0000,  0.0000,  1.6524,\n          3.8581,  0.0000,  0.0000,  4.6054,  0.0000,  0.0000, 12.4394,  0.0000,\n          0.0000,  0.0000,  0.0000, 12.6676,  0.0000, 12.4633,  7.0098,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64., 23., 64.,  0., 64.,  5.], grad_fn=<CatBackward>)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  3.6124,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.7536,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         16.8569,  4.6349,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          3.9734,  0.0000,  5.5339,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.4105,  0.0000,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[5.]], grad_fn=<UnsqueezeBackward0>)\ntensor([66., 24., 66., 24.,  0., 24.,  0.], grad_fn=<CatBackward>)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  6.1845,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.0474,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.3978,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 16.1754,\n         17.2397,  5.6779,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.8051,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 19.3805,  0.0000,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[0.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64.,  0.,  0., 23.,  0.,  1.], grad_fn=<CatBackward>)\ntensor([[15.0448,  1.1981,  4.5461,  1.9848,  6.2570,  0.0000,  0.0000, 10.7930,\n          0.0000,  0.0000,  7.3734,  0.0000,  0.0000,  0.0000, 20.9615,  0.0000,\n         10.8520,  0.0000,  0.0000,  0.0000,  4.7871, 10.0066, 11.5203,  0.6246,\n          0.0866,  0.6717,  0.0000,  9.2722,  0.0000,  0.0000,  2.4353,  5.4947,\n          0.0000,  8.9224, 12.9772,  0.0000,  0.0382,  0.4007,  5.0107,  0.0000,\n          0.0000,  0.4666,  0.7026,  0.0000,  0.3458,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  4.1490,  0.0000,  0.0000, 12.8572,  0.0000,\n          0.0000,  0.0000,  6.1417, 11.2532,  0.0000, 13.7491,  7.1651,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\ntensor([23., 64.,  0.,  0., 23.,  0.,  8.], grad_fn=<CatBackward>)\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 5.1576, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4816, 1.7860, 0.0000,\n         0.0000, 0.0000, 0.0000, 5.2006, 9.1886, 0.6774, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 4.1263, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0352,\n         0.0000]], grad_fn=<ReluBackward0>)\ntensor([[8.]], grad_fn=<UnsqueezeBackward0>)\ntensor([66., 24., 66.,  0.,  0., 24.,  8.], grad_fn=<CatBackward>)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000, 22.5421,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.5240,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 27.3578,\n         31.6320,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  5.0407,  0.0000,\n          0.0000,  0.0000, 11.6495,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 44.2643,  0.0000,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[8.]], grad_fn=<UnsqueezeBackward0>)\ntensor([74., 18.,  0., 18., 74.,  0.,  5.], grad_fn=<CatBackward>)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.3872,  0.0000,  0.0000,  3.9134,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  8.5643,  0.0000,  0.0000, 15.9620,  0.0000,\n          0.3463,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  7.2091,\n          0.0000,  7.4428,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         26.7141,  2.8118,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          5.9819,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  3.3816,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[5.]], grad_fn=<UnsqueezeBackward0>)\ntensor([19., 55., 19.,  0., 19., 55.,  0.], grad_fn=<CatBackward>)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          1.9332,  0.0000,  0.0000, 14.2768,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  2.7166,  0.0000,  0.0000, 14.1591,  0.0000,\n          0.0000,  0.0000,  0.0000,  1.7544,  0.0000,  0.0000,  0.0000,  2.9141,\n          0.0000,  3.0750,  6.3875,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         14.5275,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.2397,  2.6282,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  6.4240,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[0.]], grad_fn=<UnsqueezeBackward0>)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          3.7222,  0.0000,  0.0000, 11.1858,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  5.7257,  0.0000,  0.0000,  9.8710,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.2252,\n          0.0000,  7.0977,  1.3733,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         13.7126,  0.1994,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          1.2929,  0.2856,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  5.4654,  0.0000]],\n       grad_fn=<ReluBackward0>)\ntensor([[1.]], grad_fn=<UnsqueezeBackward0>)\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 1]) and output[0] has a shape of torch.Size([9]).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-230-8b717da7a9bb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mcur_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-229-214538a40fe3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;31m# ========================================================================= #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-229-214538a40fe3>\u001b[0m in \u001b[0;36m_train_actor\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# Compute actor gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_model_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mprediction_critic_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m# Set up relation between actor grads and weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/euclid/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mgrad_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mgrad_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/euclid/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 raise RuntimeError(\"Mismatch in shape: grad_output[\"\n\u001b[0m\u001b[1;32m     27\u001b[0m                                    \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"] has a shape of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                                    \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" and output[\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 1]) and output[0] has a shape of torch.Size([9])."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('euclid': conda)",
   "language": "python",
   "name": "python38364biteuclidconda0c4f3e6856524f32859d293db3eeb7e9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}