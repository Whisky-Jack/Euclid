{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from itertools import count\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\"\"\"\n",
    "Actions:\n",
    "    -Draw vertex\n",
    "    -Measure distance\n",
    "    -Measure angle\n",
    "\"\"\"\n",
    "# DEFINE ENVIRONMENT\n",
    "class GeometryEnvironment():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def place_vertex(self, bead_index, vertex_index):\n",
    "        self.bead_set[bead_index] = vertex_index\n",
    "\n",
    "    def measure_distance(self, i, j):\n",
    "        u = self.vertex_set[i]\n",
    "        v = self.vertex_set[j]\n",
    "        return euclidean(u, v)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.bead_set = np.array([0 for i in range(2)])\n",
    "        self.defining_state = (randint(low=1, high=100), randint(low=1, high=100))\n",
    "        x = self.defining_state\n",
    "        self.vertex_set = np.array([(0,0), (x[0],0), (0,x[1]), x])\n",
    "        self.answer = 0\n",
    "        self.correct_answer = np.sqrt(self.defining_state[0]**2 + self.defining_state[1]**2)\n",
    "    \n",
    "    def step(self, action_index):\n",
    "        # if distance should be measured\n",
    "        #print(\"beads at start of step\", self.bead_set)\n",
    "        if (action_index == 8):\n",
    "            #print(self.bead_set[0])\n",
    "            #print(self.bead_set[1])\n",
    "            print(\"HEH\")\n",
    "            self.answer = self.measure_distance(self.bead_set[0], self.bead_set[1])\n",
    "            print(self.answer)\n",
    "        else:\n",
    "            vertex_index = action_index % 4\n",
    "            if (action_index < 5):\n",
    "                # move first bead\n",
    "                self.place_vertex(0, vertex_index)\n",
    "            else:\n",
    "                self.place_vertex(1, vertex_index)\n",
    "        reward = 1 / (np.abs(self.answer - self.correct_answer)**2)#self.measure_distance(self.bead_set[0], self.bead_set[1])\n",
    "        done = self.answer == self.correct_answer\n",
    "        print(\"Answer: \", self.answer)\n",
    "        print(\"Correct answer: \", self.correct_answer)\n",
    "        print(done)\n",
    "        #print(\"beads at end of step\", self.bead_set)\n",
    "        #print(\"distance: \", self.measure_distance(self.bead_set[0], self.bead_set[1]))\n",
    "        return reward, done\n",
    "    \n",
    "    def get_state(self):\n",
    "        a = np.array(self.defining_state)\n",
    "        b = np.array(self.vertex_set[(self.bead_set[0])])\n",
    "        c = np.array(self.vertex_set[self.bead_set[1]])\n",
    "        nump = np.concatenate((a, b, c))\n",
    "        t_state = torch.tensor(nump, dtype=torch.long)\n",
    "        return t_state.flatten()\n",
    "    \n",
    "    def render(self):\n",
    "        #print(\"Answer: \", self.answer)\n",
    "        vertices = np.array([self.vertex_set[(self.bead_set[0])], self.vertex_set[(self.bead_set[1])]])\n",
    "        #print(vertices)\n",
    "        x, y = vertices.T\n",
    "        plt.scatter(x, y)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "env = GeometryEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLAY MEMORY DEFINITION\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems:\n",
    "\n",
    "-Need to provide float args to vertex position (x and y)\n",
    "\n",
    "-Need to provide index argument to vertex position\n",
    "\n",
    "-Need to provide index arguments to distance measurement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nbatch_size = 5\\nnb_classes = 2\\nin_features = 10\\n\\nmodel = nn.Linear(in_features, nb_classes)\\ncriterion = nn.CrossEntropyLoss()\\n'"
     },
     "metadata": {},
     "execution_count": 238
    }
   ],
   "source": [
    "# DEFINE THE NETWORK\n",
    "# TODO: replace CNN with linear learner\n",
    "\"\"\"\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization\n",
    "    # Returns a distribution over the possible actions tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))\n",
    "\"\"\"\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(6, 50)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(50, 30)\n",
    "        self.head = nn.Linear(30, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization\n",
    "    # Returns a distribution over the possible actions tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.head(x.view(-1, 30)) # self.head(x.view(x.size(0), -1))\n",
    "\n",
    "\"\"\"\n",
    "batch_size = 5\n",
    "nb_classes = 2\n",
    "in_features = 10\n",
    "\n",
    "model = nn.Linear(in_features, nb_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE TRANSITION OPERATIONS\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = 9\n",
    "\n",
    "policy_net = DQN(n_actions).to(device)\n",
    "target_net = DQN(n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            \"\"\"\n",
    "            best_action = policy_net(state)\n",
    "            best_action = best_action.max(1)\n",
    "            best_action = best_action[1]\n",
    "            best_action.view(1, 1)\n",
    "            \"\"\"\n",
    "            return  policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    \"\"\"\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\ntensor([34, 91,  0,  0,  0,  0])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n1\ntensor([34, 91,  0,  0,  0,  0])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n2\ntensor([34, 91, 34, 91,  0,  0])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n3\ntensor([34, 91,  0,  0,  0,  0])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n4\ntensor([34, 91,  0,  0,  0,  0])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n5\ntensor([34, 91,  0,  0, 34, 91])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n6\ntensor([34, 91,  0,  0,  0, 91])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n7\ntensor([34, 91,  0,  0, 34, 91])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n8\ntensor([34, 91,  0,  0, 34, 91])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n9\ntensor([34, 91,  0,  0, 34, 91])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n10\ntensor([34, 91,  0,  0, 34, 91])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n11\ntensor([34, 91,  0,  0, 34, 91])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n12\ntensor([34, 91, 34,  0, 34, 91])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n13\ntensor([34, 91,  0,  0, 34, 91])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n14\ntensor([34, 91, 34, 91, 34, 91])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n15\ntensor([34, 91, 34, 91,  0, 91])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n16\ntensor([34, 91, 34, 91,  0, 91])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n17\ntensor([34, 91, 34, 91,  0, 91])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n18\ntensor([34, 91, 34, 91,  0, 91])\nAnswer:  0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0001], dtype=torch.float64)\nDone:  False\n19\ntensor([34, 91,  0,  0,  0, 91])\nHEH\n91.0\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n20\ntensor([34, 91,  0,  0,  0, 91])\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n21\ntensor([34, 91,  0,  0,  0, 91])\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n22\ntensor([34, 91,  0, 91,  0, 91])\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n23\ntensor([34, 91, 34, 91,  0, 91])\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n24\ntensor([34, 91, 34, 91,  0, 91])\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n25\ntensor([34, 91,  0,  0,  0, 91])\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n26\ntensor([34, 91,  0,  0,  0, 91])\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n27\ntensor([34, 91,  0,  0, 34, 91])\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n28\ntensor([34, 91,  0,  0, 34,  0])\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n29\ntensor([34, 91,  0, 91, 34,  0])\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n30\ntensor([34, 91, 34, 91, 34,  0])\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n31\ntensor([34, 91,  0,  0, 34,  0])\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n32\ntensor([34, 91,  0, 91, 34,  0])\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n33\ntensor([34, 91,  0,  0, 34,  0])\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n34\ntensor([34, 91,  0,  0, 34,  0])\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n35\ntensor([34, 91,  0,  0, 34,  0])\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n36\ntensor([34, 91,  0,  0, 34, 91])\nAnswer:  91.0\nCorrect answer:  97.14422267947796\nFalse\nReward:  tensor([0.0265], dtype=torch.float64)\nDone:  False\n37\ntensor([34, 91,  0,  0, 34, 91])\nHEH\n97.14422267947796\nAnswer:  97.14422267947796\nCorrect answer:  97.14422267947796\nTrue\nReward:  tensor([inf], dtype=torch.float64)\nDone:  True\nDONE!\n0\ntensor([66, 35,  0,  0,  0,  0])\nAnswer:  0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0002], dtype=torch.float64)\nDone:  False\n1\ntensor([66, 35, 66, 35,  0,  0])\nAnswer:  0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0002], dtype=torch.float64)\nDone:  False\n2\ntensor([66, 35, 66,  0,  0,  0])\nAnswer:  0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0002], dtype=torch.float64)\nDone:  False\n3\ntensor([66, 35, 66, 35,  0,  0])\nAnswer:  0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0002], dtype=torch.float64)\nDone:  False\n4\ntensor([66, 35, 66, 35,  0, 35])\nAnswer:  0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0002], dtype=torch.float64)\nDone:  False\n5\ntensor([66, 35,  0, 35,  0, 35])\nAnswer:  0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0002], dtype=torch.float64)\nDone:  False\n6\ntensor([66, 35, 66, 35,  0, 35])\nHEH\n66.0\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n7\ntensor([66, 35, 66, 35,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n8\ntensor([66, 35, 66,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n9\ntensor([66, 35, 66,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n10\ntensor([66, 35,  0,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n11\ntensor([66, 35,  0,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n12\ntensor([66, 35, 66,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n13\ntensor([66, 35, 66,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n14\ntensor([66, 35, 66,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n15\ntensor([66, 35, 66,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n16\ntensor([66, 35, 66,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n17\ntensor([66, 35, 66,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n18\ntensor([66, 35, 66,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n19\ntensor([66, 35, 66,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n20\ntensor([66, 35, 66,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n21\ntensor([66, 35, 66,  0, 66,  0])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n22\ntensor([66, 35, 66,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n23\ntensor([66, 35, 66,  0, 66, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n24\ntensor([66, 35, 66,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n25\ntensor([66, 35,  0,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n26\ntensor([66, 35,  0,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n27\ntensor([66, 35,  0,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n28\ntensor([66, 35,  0,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n29\ntensor([66, 35,  0,  0, 66,  0])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n30\ntensor([66, 35,  0,  0, 66,  0])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n31\ntensor([66, 35,  0,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n32\ntensor([66, 35,  0,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n33\ntensor([66, 35,  0,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n34\ntensor([66, 35,  0,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n35\ntensor([66, 35, 66,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n36\ntensor([66, 35, 66,  0,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n37\ntensor([66, 35, 66, 35,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n38\ntensor([66, 35, 66, 35, 66,  0])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n39\ntensor([66, 35, 66, 35,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n40\ntensor([66, 35, 66, 35,  0, 35])\nAnswer:  66.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0132], dtype=torch.float64)\nDone:  False\n41\ntensor([66, 35,  0,  0,  0, 35])\nHEH\n35.0\nAnswer:  35.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0006], dtype=torch.float64)\nDone:  False\n42\ntensor([66, 35,  0,  0,  0, 35])\nHEH\n35.0\nAnswer:  35.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0006], dtype=torch.float64)\nDone:  False\n43\ntensor([66, 35,  0,  0,  0, 35])\nAnswer:  35.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0006], dtype=torch.float64)\nDone:  False\n44\ntensor([66, 35,  0,  0,  0, 35])\nAnswer:  35.0\nCorrect answer:  74.70609078247904\nFalse\nReward:  tensor([0.0006], dtype=torch.float64)\nDone:  False\n45\ntensor([66, 35, 66,  0,  0, 35])\nHEH\n74.70609078247904\nAnswer:  74.70609078247904\nCorrect answer:  74.70609078247904\nTrue\nReward:  tensor([inf], dtype=torch.float64)\nDone:  True\nDONE!\n0\ntensor([51, 13,  0,  0,  0,  0])\nAnswer:  0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n1\ntensor([51, 13,  0,  0, 51,  0])\nAnswer:  0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n2\ntensor([51, 13,  0,  0, 51,  0])\nAnswer:  0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n3\ntensor([51, 13,  0,  0, 51,  0])\nAnswer:  0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n4\ntensor([51, 13,  0,  0, 51,  0])\nAnswer:  0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n5\ntensor([51, 13,  0,  0, 51,  0])\nAnswer:  0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n6\ntensor([51, 13,  0,  0, 51,  0])\nAnswer:  0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n7\ntensor([51, 13,  0,  0, 51,  0])\nAnswer:  0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n8\ntensor([51, 13,  0,  0, 51,  0])\nAnswer:  0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n9\ntensor([51, 13,  0, 13, 51,  0])\nAnswer:  0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n10\ntensor([51, 13,  0,  0, 51,  0])\nAnswer:  0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n11\ntensor([51, 13,  0,  0,  0, 13])\nAnswer:  0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n12\ntensor([51, 13,  0,  0,  0, 13])\nHEH\n13.0\nAnswer:  13.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0006], dtype=torch.float64)\nDone:  False\n13\ntensor([51, 13,  0,  0,  0, 13])\nAnswer:  13.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0006], dtype=torch.float64)\nDone:  False\n14\ntensor([51, 13,  0, 13,  0, 13])\nAnswer:  13.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0006], dtype=torch.float64)\nDone:  False\n15\ntensor([51, 13,  0, 13, 51,  0])\nAnswer:  13.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0006], dtype=torch.float64)\nDone:  False\n16\ntensor([51, 13,  0,  0, 51,  0])\nAnswer:  13.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0006], dtype=torch.float64)\nDone:  False\n17\ntensor([51, 13,  0,  0, 51,  0])\nAnswer:  13.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0006], dtype=torch.float64)\nDone:  False\n18\ntensor([51, 13,  0,  0, 51,  0])\nAnswer:  13.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0006], dtype=torch.float64)\nDone:  False\n19\ntensor([51, 13,  0,  0, 51,  0])\nAnswer:  13.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0006], dtype=torch.float64)\nDone:  False\n20\ntensor([51, 13,  0,  0, 51,  0])\nHEH\n51.0\nAnswer:  51.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.3760], dtype=torch.float64)\nDone:  False\n21\ntensor([51, 13,  0,  0, 51,  0])\nAnswer:  51.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.3760], dtype=torch.float64)\nDone:  False\n22\ntensor([51, 13,  0,  0,  0, 13])\nAnswer:  51.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.3760], dtype=torch.float64)\nDone:  False\n23\ntensor([51, 13,  0,  0,  0, 13])\nAnswer:  51.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.3760], dtype=torch.float64)\nDone:  False\n24\ntensor([51, 13,  0,  0, 51, 13])\nAnswer:  51.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.3760], dtype=torch.float64)\nDone:  False\n25\ntensor([51, 13, 51, 13, 51, 13])\nAnswer:  51.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.3760], dtype=torch.float64)\nDone:  False\n26\ntensor([51, 13, 51, 13, 51, 13])\nAnswer:  51.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.3760], dtype=torch.float64)\nDone:  False\n27\ntensor([51, 13, 51, 13, 51, 13])\nAnswer:  51.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.3760], dtype=torch.float64)\nDone:  False\n28\ntensor([51, 13, 51, 13,  0, 13])\nAnswer:  51.0\nCorrect answer:  52.630789467763066\nFalse\nReward:tensor([0.3760], dtype=torch.float64)\nDone:  False\n29\ntensor([51, 13, 51, 13,  0, 13])\nAnswer:  51.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.3760], dtype=torch.float64)\nDone:  False\n30\ntensor([51, 13, 51, 13,  0, 13])\nAnswer:  51.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.3760], dtype=torch.float64)\nDone:  False\n31\ntensor([51, 13, 51, 13, 51,  0])\nAnswer:  51.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.3760], dtype=torch.float64)\nDone:  False\n32\ntensor([51, 13, 51, 13, 51,  0])\nAnswer:  51.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.3760], dtype=torch.float64)\nDone:  False\n33\ntensor([51, 13, 51, 13,  0, 13])\nAnswer:  51.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.3760], dtype=torch.float64)\nDone:  False\n34\ntensor([51, 13, 51, 13,  0, 13])\nAnswer:  51.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.3760], dtype=torch.float64)\nDone:  False\n35\ntensor([51, 13, 51, 13,  0, 13])\nAnswer:  51.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.3760], dtype=torch.float64)\nDone:  False\n36\ntensor([51, 13,  0,  0,  0, 13])\nAnswer:  51.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.3760], dtype=torch.float64)\nDone:  False\n37\ntensor([51, 13,  0, 13,  0, 13])\nHEH\n0.0\nAnswer:  0.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n38\ntensor([51, 13,  0, 13,  0, 13])\nAnswer:  0.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n39\ntensor([51, 13,  0, 13, 51,  0])\nAnswer:  0.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n40\ntensor([51, 13,  0, 13, 51,  0])\nAnswer:  0.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n41\ntensor([51, 13,  0,  0, 51,  0])\nAnswer:  0.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n42\ntensor([51, 13,  0,  0, 51,  0])\nAnswer:  0.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n43\ntensor([51, 13, 51,  0, 51,  0])\nAnswer:  0.0\nCorrect answer:  52.630789467763066\nFalse\nReward:  tensor([0.0004], dtype=torch.float64)\nDone:  False\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 768], m2: [6 x 50] at /opt/conda/conda-bld/pytorch_1587428207430/work/aten/src/TH/generic/THTensorMath.cpp:41",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-241-1e1090e501c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Perform one step of the optimization (on the target network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mepisode_durations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-240-a93141a404c5>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# columns of actions taken. These are the actions which would've been taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# for each batch state according to policy_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mstate_action_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Compute V(s_{t+1}) for all next states.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/euclid/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-238-41b9c797de4c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# self.head(x.view(x.size(0), -1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/euclid/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/euclid/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/euclid/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 768], m2: [6 x 50] at /opt/conda/conda-bld/pytorch_1587428207430/work/aten/src/TH/generic/THTensorMath.cpp:41"
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    state = env.get_state()\n",
    "    for t in count():\n",
    "        print(t)\n",
    "        print(state)\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        reward, done = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        print(\"Reward: \", reward)\n",
    "        print(\"Done: \", done)\n",
    "\n",
    "        # Observe new state\n",
    "        if not done:\n",
    "            next_state = env.get_state()\n",
    "        else:\n",
    "            print(\"DONE!\")\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "#env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('euclid': conda)",
   "language": "python",
   "name": "python38364biteuclidconda0c4f3e6856524f32859d293db3eeb7e9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}